{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Créer un LLM from Scratch\n",
    "\n",
    "Ce notebook explique pas à pas comment construire un Large Language Model (LLM) de type GPT.\n",
    "\n",
    "## Plan\n",
    "1. **Tokenization** - Convertir du texte en nombres\n",
    "2. **Embeddings** - Représenter les tokens dans un espace vectoriel\n",
    "3. **Attention** - Le mécanisme clé des Transformers\n",
    "4. **Architecture GPT** - Assembler les blocs\n",
    "5. **Entraînement** - Apprendre à prédire le prochain token\n",
    "6. **Génération** - Produire du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Imports nécessaires\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import regex as re\n",
    "\n",
    "# Vérifie le device disponible\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Mac M1/M2/M3\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 0. Téléchargement des données (Wikipedia FR)\n",
    "\n",
    "Avant de commencer, on télécharge un corpus de texte français depuis Wikipedia.\n",
    "\n",
    "**Options disponibles:**\n",
    "- `small` : ~100MB, ~50 000 articles (recommandé pour débuter)\n",
    "- `medium` : ~500MB, ~250 000 articles\n",
    "- `large` : ~2GB, ~1 000 000 articles\n",
    "\n",
    "Le téléchargement peut prendre quelques minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_wikipedia_fr(size: str = \"small\", output_dir: str = \"data\"):\n",
    "    \"\"\"\n",
    "    Télécharge Wikipedia français.\n",
    "    \n",
    "    Args:\n",
    "        size: \"small\" (~100MB), \"medium\" (~500MB), \"large\" (~2GB)\n",
    "        output_dir: Dossier de sortie\n",
    "    \"\"\"\n",
    "    n_articles = {\"small\": 50_000, \"medium\": 250_000, \"large\": 1_000_000}\n",
    "    n = n_articles.get(size, 50_000)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"wikipedia_fr_{size}.txt\")\n",
    "    \n",
    "    # Vérifie si déjà téléchargé\n",
    "    if os.path.exists(output_path):\n",
    "        size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "        print(f\"✓ Données déjà téléchargées: {output_path} ({size_mb:.1f} MB)\")\n",
    "        return output_path\n",
    "    \n",
    "    print(f\"Téléchargement de Wikipedia FR ({size}: {n:,} articles)...\")\n",
    "    print(\"Cela peut prendre quelques minutes...\\n\")\n",
    "    \n",
    "    # Charge le dataset en streaming\n",
    "    dataset = load_dataset(\n",
    "        \"wikimedia/wikipedia\",\n",
    "        \"20231101.fr\",\n",
    "        split=\"train\",\n",
    "        streaming=True\n",
    "    )\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    total_chars = 0\n",
    "    count = 0\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for article in tqdm(dataset, total=n, desc=\"Téléchargement\"):\n",
    "            if count >= n:\n",
    "                break\n",
    "            \n",
    "            text = article[\"text\"]\n",
    "            if len(text) < 500:  # Ignore les articles trop courts\n",
    "                continue\n",
    "            \n",
    "            f.write(f\"# {article['title']}\\n\\n\")\n",
    "            f.write(text)\n",
    "            f.write(\"\\n\\n---\\n\\n\")\n",
    "            \n",
    "            total_chars += len(text)\n",
    "            count += 1\n",
    "    \n",
    "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    print(f\"\\n✓ Téléchargement terminé!\")\n",
    "    print(f\"  Fichier: {output_path}\")\n",
    "    print(f\"  Taille: {size_mb:.1f} MB\")\n",
    "    print(f\"  Articles: {count:,}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Télécharge les données (change \"small\" en \"medium\" ou \"large\" si tu veux plus de données)\n",
    "DATA_PATH = download_wikipedia_fr(size=\"small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Tokenization (BPE)\n",
    "\n",
    "## Pourquoi tokenizer ?\n",
    "\n",
    "Un modèle ne comprend pas le texte directement. Il faut convertir les mots en **nombres**.\n",
    "\n",
    "### Approches possibles :\n",
    "\n",
    "| Méthode | Exemple \"hello\" | Problème |\n",
    "|---------|-----------------|----------|\n",
    "| Par caractère | `[h, e, l, l, o]` → `[104, 101, 108, 108, 111]` | Séquences très longues |\n",
    "| Par mot | `[hello]` → `[2847]` | Vocabulaire énorme, mots inconnus |\n",
    "| **BPE (sous-mots)** | `[hel, lo]` → `[892, 341]` | Bon compromis ✓ |\n",
    "\n",
    "## Comment fonctionne BPE ?\n",
    "\n",
    "1. Commence avec les 256 bytes comme vocabulaire de base\n",
    "2. Compte les paires de tokens adjacents les plus fréquentes\n",
    "3. Fusionne la paire la plus fréquente → nouveau token\n",
    "4. Répète jusqu'à atteindre la taille de vocabulaire souhaitée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern de pré-tokenization (sépare en mots, nombres, ponctuation)\n",
    "GPT2_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer Byte Pair Encoding simplifié.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = {}  # (token1, token2) -> nouveau_token\n",
    "        self.vocab = {}   # bytes -> id\n",
    "        self.inverse_vocab = {}  # id -> bytes\n",
    "        \n",
    "        # Tokens spéciaux\n",
    "        self.special_tokens = {\n",
    "            \"<|pad|>\": 0,\n",
    "            \"<|unk|>\": 1, \n",
    "            \"<|bos|>\": 2,  # Beginning of sequence\n",
    "            \"<|eos|>\": 3,  # End of sequence\n",
    "        }\n",
    "        \n",
    "        self.pattern = re.compile(GPT2_PATTERN)\n",
    "        self._init_base_vocab()\n",
    "    \n",
    "    def _init_base_vocab(self):\n",
    "        \"\"\"Initialise avec les 256 bytes + tokens spéciaux.\"\"\"\n",
    "        # Tokens spéciaux d'abord\n",
    "        for token, idx in self.special_tokens.items():\n",
    "            self.vocab[token.encode()] = idx\n",
    "            self.inverse_vocab[idx] = token.encode()\n",
    "        \n",
    "        # Puis les 256 bytes possibles\n",
    "        for i in range(256):\n",
    "            byte = bytes([i])\n",
    "            idx = i + len(self.special_tokens)\n",
    "            self.vocab[byte] = idx\n",
    "            self.inverse_vocab[idx] = byte\n",
    "    \n",
    "    def _get_stats(self, token_ids: list[list[int]]) -> dict:\n",
    "        \"\"\"Compte la fréquence de chaque paire adjacente.\"\"\"\n",
    "        stats = defaultdict(int)\n",
    "        for ids in token_ids:\n",
    "            for i in range(len(ids) - 1):\n",
    "                pair = (ids[i], ids[i + 1])\n",
    "                stats[pair] += 1\n",
    "        return stats\n",
    "    \n",
    "    def _merge(self, token_ids: list[list[int]], pair: tuple, new_id: int) -> list[list[int]]:\n",
    "        \"\"\"Fusionne toutes les occurrences d'une paire.\"\"\"\n",
    "        new_token_ids = []\n",
    "        for ids in token_ids:\n",
    "            new_ids = []\n",
    "            i = 0\n",
    "            while i < len(ids):\n",
    "                if i < len(ids) - 1 and (ids[i], ids[i + 1]) == pair:\n",
    "                    new_ids.append(new_id)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_ids.append(ids[i])\n",
    "                    i += 1\n",
    "            new_token_ids.append(new_ids)\n",
    "        return new_token_ids\n",
    "    \n",
    "    def train(self, text: str, verbose: bool = True):\n",
    "        \"\"\"Entraîne le tokenizer sur un corpus.\"\"\"\n",
    "        # Pré-tokenization\n",
    "        chunks = self.pattern.findall(text)\n",
    "        \n",
    "        # Convertit en bytes\n",
    "        token_ids = []\n",
    "        for chunk in chunks:\n",
    "            ids = [self.vocab[bytes([b])] for b in chunk.encode()]\n",
    "            token_ids.append(ids)\n",
    "        \n",
    "        n_merges = self.vocab_size - len(self.vocab)\n",
    "        if verbose:\n",
    "            print(f\"Entraînement: {n_merges} merges à effectuer\")\n",
    "        \n",
    "        for i in range(n_merges):\n",
    "            stats = self._get_stats(token_ids)\n",
    "            if not stats:\n",
    "                break\n",
    "            \n",
    "            # Trouve la paire la plus fréquente\n",
    "            best_pair = max(stats, key=stats.get)\n",
    "            if stats[best_pair] < 2:\n",
    "                break\n",
    "            \n",
    "            # Crée un nouveau token\n",
    "            new_id = len(self.vocab)\n",
    "            self.merges[best_pair] = new_id\n",
    "            \n",
    "            # Concatène les bytes\n",
    "            new_token = self.inverse_vocab[best_pair[0]] + self.inverse_vocab[best_pair[1]]\n",
    "            self.vocab[new_token] = new_id\n",
    "            self.inverse_vocab[new_id] = new_token\n",
    "            \n",
    "            # Applique le merge\n",
    "            token_ids = self._merge(token_ids, best_pair, new_id)\n",
    "            \n",
    "            if verbose and (i + 1) % 100 == 0:\n",
    "                print(f\"  Merge {i + 1}/{n_merges}\")\n",
    "        \n",
    "        print(f\"Vocabulaire final: {len(self.vocab)} tokens\")\n",
    "    \n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"Encode du texte en IDs.\"\"\"\n",
    "        chunks = self.pattern.findall(text)\n",
    "        all_ids = [self.special_tokens[\"<|bos|>\"]]\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            ids = [self.vocab[bytes([b])] for b in chunk.encode()]\n",
    "            # Applique les merges\n",
    "            for pair, new_id in self.merges.items():\n",
    "                ids = self._merge([ids], pair, new_id)[0]\n",
    "            all_ids.extend(ids)\n",
    "        \n",
    "        all_ids.append(self.special_tokens[\"<|eos|>\"])\n",
    "        return all_ids\n",
    "    \n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        \"\"\"Décode des IDs en texte.\"\"\"\n",
    "        special_ids = set(self.special_tokens.values())\n",
    "        byte_list = []\n",
    "        for id_ in ids:\n",
    "            if id_ not in special_ids and id_ in self.inverse_vocab:\n",
    "                byte_list.append(self.inverse_vocab[id_])\n",
    "        return b\"\".join(byte_list).decode(\"utf-8\", errors=\"replace\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données Wikipedia pour le tokenizer...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DATA_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mChargement des données Wikipedia pour le tokenizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# On utilise une partie des données pour entraîner le tokenizer (plus rapide)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mDATA_PATH\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Lit les premiers 10MB pour le tokenizer (suffisant pour apprendre le vocabulaire)\u001b[39;00m\n\u001b[32m      7\u001b[39m     texte_tokenizer = f.read(\u001b[32m10_000_000\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTexte chargé: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texte_tokenizer)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m caractères\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'DATA_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# Entraîne le tokenizer sur les données Wikipedia\n",
    "print(\"Chargement des données Wikipedia pour le tokenizer...\")\n",
    "\n",
    "# On utilise une partie des données pour entraîner le tokenizer (plus rapide)\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    # Lit les premiers 10MB pour le tokenizer (suffisant pour apprendre le vocabulaire)\n",
    "    texte_tokenizer = f.read(10_000_000)\n",
    "\n",
    "print(f\"Texte chargé: {len(texte_tokenizer):,} caractères\")\n",
    "\n",
    "# Entraîne le tokenizer avec un vocabulaire plus grand pour Wikipedia\n",
    "tokenizer = BPETokenizer(vocab_size=4000)\n",
    "tokenizer.train(texte_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encode/decode\n",
    "texte_test = \"Bonjour, comment ça va ?\"\n",
    "\n",
    "encoded = tokenizer.encode(texte_test)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"Texte original : {texte_test}\")\n",
    "print(f\"Encodé (IDs)   : {encoded}\")\n",
    "print(f\"Décodé         : {decoded}\")\n",
    "print(f\"\\nNombre de tokens: {len(encoded)} (vs {len(texte_test)} caractères)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Embeddings\n",
    "\n",
    "## Pourquoi des embeddings ?\n",
    "\n",
    "Les IDs de tokens sont juste des nombres (0, 1, 2...). Le modèle a besoin de **vecteurs** pour calculer.\n",
    "\n",
    "Un **embedding** transforme chaque ID en un vecteur de dimension `d_model` (par ex. 384).\n",
    "\n",
    "```\n",
    "Token ID: 42  →  Embedding: [0.12, -0.45, 0.78, ..., 0.33]  (384 dimensions)\n",
    "```\n",
    "\n",
    "Ces vecteurs sont **appris** pendant l'entraînement. Des mots similaires auront des vecteurs proches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple d'embedding\n",
    "vocab_size = 500\n",
    "d_model = 384  # Dimension des vecteurs\n",
    "\n",
    "# Crée une table d'embedding (matrice vocab_size x d_model)\n",
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "# Exemple: convertit des IDs en vecteurs\n",
    "token_ids = torch.tensor([10, 42, 100])  # 3 tokens\n",
    "vectors = embedding(token_ids)\n",
    "\n",
    "print(f\"Input shape  : {token_ids.shape}  (3 tokens)\")\n",
    "print(f\"Output shape : {vectors.shape}  (3 vecteurs de {d_model} dimensions)\")\n",
    "print(f\"\\nVecteur du token 42:\\n{vectors[1][:10]}...\")  # Premiers éléments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Positional Encoding (RoPE)\n",
    "\n",
    "## Le problème\n",
    "\n",
    "L'attention traite tous les tokens **en parallèle**. Sans information de position, le modèle ne sait pas quel mot vient avant l'autre !\n",
    "\n",
    "\"Le chat mange la souris\" vs \"La souris mange le chat\" → même embeddings !\n",
    "\n",
    "## Solution: RoPE (Rotary Position Embedding)\n",
    "\n",
    "RoPE encode la position par une **rotation** dans l'espace des vecteurs. Plus moderne et efficace que les positional encodings classiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    RoPE: encode les positions par rotation.\n",
    "    Utilisé dans LLaMA, Mistral, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, max_seq_len: int = 2048, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Calcul des fréquences de rotation\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "        # Pré-calcul des cos et sin pour chaque position\n",
    "        positions = torch.arange(max_seq_len)\n",
    "        freqs = torch.outer(positions, inv_freq)\n",
    "        self.register_buffer(\"cos_cached\", freqs.cos())\n",
    "        self.register_buffer(\"sin_cached\", freqs.sin())\n",
    "    \n",
    "    def forward(self, seq_len: int):\n",
    "        return self.cos_cached[:seq_len], self.sin_cached[:seq_len]\n",
    "\n",
    "\n",
    "def apply_rotary_emb(q, k, cos, sin):\n",
    "    \"\"\"Applique la rotation à Q et K.\"\"\"\n",
    "    # Sépare en deux moitiés\n",
    "    q1, q2 = q[..., ::2], q[..., 1::2]\n",
    "    k1, k2 = k[..., ::2], k[..., 1::2]\n",
    "    \n",
    "    # Reshape pour broadcasting\n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Rotation: (a + bi) * (cos + i*sin) = (a*cos - b*sin) + i*(a*sin + b*cos)\n",
    "    q_rot = torch.cat([q1 * cos - q2 * sin, q1 * sin + q2 * cos], dim=-1)\n",
    "    k_rot = torch.cat([k1 * cos - k2 * sin, k1 * sin + k2 * cos], dim=-1)\n",
    "    \n",
    "    return q_rot, k_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des fréquences RoPE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rope = RotaryPositionalEmbedding(dim=64, max_seq_len=100)\n",
    "cos, sin = rope(100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].imshow(cos.numpy(), aspect='auto', cmap='coolwarm')\n",
    "axes[0].set_title('Cosinus (positions x dimensions)')\n",
    "axes[0].set_xlabel('Dimension')\n",
    "axes[0].set_ylabel('Position')\n",
    "\n",
    "axes[1].imshow(sin.numpy(), aspect='auto', cmap='coolwarm')\n",
    "axes[1].set_title('Sinus (positions x dimensions)')\n",
    "axes[1].set_xlabel('Dimension')\n",
    "axes[1].set_ylabel('Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Self-Attention\n",
    "\n",
    "## L'idée clé\n",
    "\n",
    "L'attention permet à chaque token de \"regarder\" les autres tokens pour comprendre le contexte.\n",
    "\n",
    "**Exemple:** Dans \"Le chat dort sur le canapé\"\n",
    "- \"dort\" doit regarder \"chat\" pour savoir QUI dort\n",
    "- \"canapé\" doit regarder \"sur\" pour comprendre la relation spatiale\n",
    "\n",
    "## Mécanisme Q, K, V\n",
    "\n",
    "Pour chaque token, on calcule 3 vecteurs:\n",
    "- **Q (Query)**: \"Que cherche ce token ?\"\n",
    "- **K (Key)**: \"Qu'est-ce que ce token offre ?\"\n",
    "- **V (Value)**: \"Quelle information ce token contient ?\"\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q @ K^T / √d) @ V\n",
    "```\n",
    "\n",
    "## Attention Causale (pour GPT)\n",
    "\n",
    "Un token ne peut voir que les tokens **précédents** (pas le futur). On utilise un masque triangulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention avec masque causal.\n",
    "    \n",
    "    \"Multi-Head\" = on fait plusieurs attentions en parallèle,\n",
    "    chacune se spécialisant sur différents aspects.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, max_seq_len: int = 256, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        assert d_model % n_heads == 0, \"d_model doit être divisible par n_heads\"\n",
    "        \n",
    "        # Projections linéaires pour Q, K, V et Output\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rope = RotaryPositionalEmbedding(self.head_dim, max_seq_len)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # 1. Projections linéaires\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        # 2. Reshape pour multi-head: (batch, seq, n_heads, head_dim)\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        # Shape maintenant: (batch, n_heads, seq, head_dim)\n",
    "        \n",
    "        # 3. Applique RoPE (positional encoding)\n",
    "        cos, sin = self.rope(seq_len)\n",
    "        q, k = apply_rotary_emb(q, k, cos, sin)\n",
    "        \n",
    "        # 4. Calcul de l'attention: Q @ K^T / sqrt(d)\n",
    "        scale = math.sqrt(self.head_dim)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / scale\n",
    "        # Shape: (batch, n_heads, seq, seq)\n",
    "        \n",
    "        # 5. Masque causal (triangle inférieur)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "        scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "        \n",
    "        # 6. Softmax et dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # 7. Attention @ V\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        # Shape: (batch, n_heads, seq, head_dim)\n",
    "        \n",
    "        # 8. Recombine les heads\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # 9. Projection de sortie\n",
    "        return self.o_proj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du masque causal\n",
    "seq_len = 8\n",
    "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(1 - mask, cmap='Blues')\n",
    "plt.title(\"Masque Causal\\n(blanc = peut voir, bleu = masqué)\")\n",
    "plt.xlabel(\"Position K (clé)\")\n",
    "plt.ylabel(\"Position Q (requête)\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Ajoute les labels\n",
    "tokens = [\"Le\", \"chat\", \"dort\", \"sur\", \"le\", \"canapé\", \".\", \"<eos>\"]\n",
    "plt.xticks(range(seq_len), tokens, rotation=45)\n",
    "plt.yticks(range(seq_len), tokens)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Exemple: 'dort' (position 2) peut voir 'Le', 'chat', 'dort'\")\n",
    "print(\"         mais pas 'sur', 'le', 'canapé'...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de l'attention\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "attn = MultiHeadAttention(d_model, n_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output = attn(x)\n",
    "print(f\"Input shape : {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Feed-Forward Network (SwiGLU)\n",
    "\n",
    "Après l'attention, chaque token passe par un réseau feed-forward.\n",
    "\n",
    "**SwiGLU** est une activation moderne (utilisée dans LLaMA, PaLM):\n",
    "```\n",
    "SwiGLU(x) = Swish(xW1) * (xW3)\n",
    "```\n",
    "\n",
    "Plus efficace que ReLU ou GELU classique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-Forward avec activation SwiGLU.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Pour SwiGLU, on ajuste la dimension cachée\n",
    "        hidden_dim = int(2 * d_ff / 3)\n",
    "        hidden_dim = ((hidden_dim + 7) // 8) * 8  # Multiple de 8 (optimisation GPU)\n",
    "        \n",
    "        self.w1 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, d_model, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, hidden_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # SwiGLU: swish(xW1) * (xW3)\n",
    "        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des activations\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x, F.relu(x), label='ReLU')\n",
    "plt.plot(x, F.gelu(x), label='GELU')\n",
    "plt.plot(x, F.silu(x), label='SiLU/Swish')\n",
    "plt.legend()\n",
    "plt.title('Fonctions d\\'activation')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('activation(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. RMSNorm\n",
    "\n",
    "La normalisation stabilise l'entraînement. **RMSNorm** est plus simple que LayerNorm:\n",
    "\n",
    "```\n",
    "RMSNorm(x) = x / RMS(x) * γ\n",
    "où RMS(x) = sqrt(mean(x²))\n",
    "```\n",
    "\n",
    "Pas besoin de soustraire la moyenne → plus rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))  # γ (learnable)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return (x / rms) * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Transformer Block\n",
    "\n",
    "Un bloc Transformer combine tous les composants:\n",
    "\n",
    "```\n",
    "x → RMSNorm → Attention → + → RMSNorm → FeedForward → + → output\n",
    "      ↑______________________|       ↑_________________|\n",
    "           (résiduel)                   (résiduel)\n",
    "```\n",
    "\n",
    "Les connexions **résiduelles** (`+`) permettent au gradient de circuler facilement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Un bloc Transformer complet.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, \n",
    "                 max_seq_len: int = 256, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, max_seq_len, dropout)\n",
    "        self.ff_norm = RMSNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Attention avec résiduel\n",
    "        x = x + self.attn(self.attn_norm(x))\n",
    "        # Feed-forward avec résiduel\n",
    "        x = x + self.ff(self.ff_norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Le Modèle GPT Complet\n",
    "\n",
    "On assemble tout:\n",
    "\n",
    "```\n",
    "Input IDs\n",
    "    ↓\n",
    "Token Embedding\n",
    "    ↓\n",
    "Transformer Block 1\n",
    "    ↓\n",
    "Transformer Block 2\n",
    "    ↓\n",
    "    ...\n",
    "    ↓\n",
    "Transformer Block N\n",
    "    ↓\n",
    "RMSNorm\n",
    "    ↓\n",
    "LM Head (projection vers vocabulaire)\n",
    "    ↓\n",
    "Logits (probabilités pour chaque token)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"Modèle GPT complet.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int = 384, n_heads: int = 6,\n",
    "                 n_layers: int = 6, d_ff: int = 1536, max_seq_len: int = 256,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Token Embedding\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, max_seq_len, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Normalisation finale\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        \n",
    "        # LM Head: projette vers le vocabulaire\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying: partage les poids embedding <-> lm_head\n",
    "        self.tok_emb.weight = self.lm_head.weight\n",
    "        \n",
    "        # Initialisation\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # Compte les paramètres\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"Paramètres: {n_params / 1e6:.2f}M\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, labels: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        input_ids: (batch, seq_len)\n",
    "        labels: (batch, seq_len) - optionnel, pour calculer la loss\n",
    "        \"\"\"\n",
    "        # Token embedding\n",
    "        x = self.tok_emb(input_ids)\n",
    "        \n",
    "        # Passe à travers les blocs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Normalisation finale\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Projection vers vocabulaire\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        # Calcul de la loss si labels fournis\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift: prédire le token suivant\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, self.vocab_size),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "        \n",
    "        return {\"logits\": logits, \"loss\": loss}\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids: torch.Tensor, max_new_tokens: int = 50,\n",
    "                 temperature: float = 1.0, top_k: int = 50):\n",
    "        \"\"\"Génère du texte auto-régressivement.\"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Tronque si trop long\n",
    "            idx = input_ids if input_ids.shape[1] <= self.max_seq_len else input_ids[:, -self.max_seq_len:]\n",
    "            \n",
    "            # Forward\n",
    "            logits = self(idx)[\"logits\"][:, -1, :]  # Dernier token\n",
    "            \n",
    "            # Température\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Top-k\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float(\"-inf\")\n",
    "            \n",
    "            # Échantillonne\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Ajoute au contexte\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crée le modèle avec des paramètres adaptés à Wikipedia\n",
    "model = GPT(\n",
    "    vocab_size=len(tokenizer),\n",
    "    d_model=384,      # Plus grand pour mieux capturer la complexité\n",
    "    n_heads=6,\n",
    "    n_layers=6,\n",
    "    d_ff=1536,\n",
    "    max_seq_len=256,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Test forward pass\n",
    "test_ids = torch.randint(0, len(tokenizer), (2, 64)).to(device)\n",
    "output = model(test_ids, labels=test_ids)\n",
    "\n",
    "print(f\"\\nInput shape : {test_ids.shape}\")\n",
    "print(f\"Logits shape: {output['logits'].shape}\")\n",
    "print(f\"Loss        : {output['loss'].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Entraînement\n",
    "\n",
    "Le modèle apprend à **prédire le prochain token** (language modeling).\n",
    "\n",
    "Pour chaque séquence:\n",
    "```\n",
    "Input:  [Le, chat, dort, sur]\n",
    "Target: [chat, dort, sur, le]   (décalé de 1)\n",
    "```\n",
    "\n",
    "On minimise la **cross-entropy** entre les prédictions et les vrais tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prépare les données d'entraînement depuis Wikipedia FR\n",
    "print(\"Chargement et tokenization des données Wikipedia...\")\n",
    "\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    texte_complet = f.read()\n",
    "\n",
    "print(f\"Texte chargé: {len(texte_complet):,} caractères ({len(texte_complet) / 1e6:.1f} MB)\")\n",
    "\n",
    "# Tokenize tout le texte (peut prendre un moment)\n",
    "print(\"Tokenization en cours...\")\n",
    "all_tokens = tokenizer.encode(texte_complet)\n",
    "print(f\"Nombre total de tokens: {len(all_tokens):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "# Crée le dossier checkpoints\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens: list[int], seq_len: int = 128):\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.tokens) - self.seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.tokens[idx:idx + self.seq_len]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(chunk, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(chunk, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Crée le dataset et dataloader\n",
    "# seq_len=128 pour des séquences plus longues (Wikipedia a des articles longs)\n",
    "dataset = TextDataset(all_tokens, seq_len=128)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "print(f\"Nombre de séquences: {len(dataset):,}\")\n",
    "print(f\"Nombre de batches: {len(dataloader):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Optimiseur\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "\n",
    "# Entraînement\n",
    "# Note: avec Wikipedia, 1-2 epochs suffisent souvent car beaucoup de données\n",
    "n_epochs = 2\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward\n",
    "        output = model(input_ids, labels=labels)\n",
    "        loss = output[\"loss\"]\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} - Loss moyenne: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Sauvegarde un checkpoint après chaque epoch\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"loss\": avg_loss,\n",
    "    }, f\"checkpoints/model_epoch_{epoch + 1}.pt\")\n",
    "    print(f\"  → Checkpoint sauvegardé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise la loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Courbe d'entraînement\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 10. Génération de Texte\n",
    "\n",
    "Maintenant qu'il est entraîné, le modèle peut générer du texte !\n",
    "\n",
    "## Paramètres de génération\n",
    "\n",
    "- **temperature**: Contrôle la \"créativité\"\n",
    "  - < 1.0 : Plus déterministe, répétitif\n",
    "  - = 1.0 : Normal\n",
    "  - > 1.0 : Plus aléatoire, créatif\n",
    "\n",
    "- **top_k**: Ne garde que les k tokens les plus probables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_tokens: int = 50, temperature: float = 0.8, top_k: int = 40):\n",
    "    \"\"\"Génère du texte à partir d'un prompt.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode le prompt\n",
    "    input_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "    \n",
    "    # Génère\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # Décode\n",
    "    return tokenizer.decode(output_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de génération avec des prompts Wikipedia\n",
    "prompts = [\n",
    "    \"La France est\",\n",
    "    \"L'histoire de\",\n",
    "    \"Paris est une ville\",\n",
    "    \"La science permet de\",\n",
    "    \"En 1789,\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    generated = generate_text(prompt, max_tokens=60, temperature=0.8)\n",
    "    print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expérimente avec la température\n",
    "prompt = \"Bonjour\"\n",
    "\n",
    "print(\"Effet de la température:\\n\")\n",
    "for temp in [0.3, 0.7, 1.0, 1.5]:\n",
    "    print(f\"Temperature = {temp}:\")\n",
    "    print(generate_text(prompt, max_tokens=30, temperature=temp))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Résumé\n",
    "\n",
    "## Ce qu'on a construit:\n",
    "\n",
    "1. **Tokenizer BPE** - Convertit texte ↔ nombres\n",
    "2. **Embeddings** - Représente les tokens en vecteurs\n",
    "3. **RoPE** - Encode les positions par rotation\n",
    "4. **Multi-Head Attention** - Permet aux tokens de \"se regarder\"\n",
    "5. **Feed-Forward (SwiGLU)** - Traitement non-linéaire\n",
    "6. **RMSNorm** - Stabilise l'entraînement\n",
    "7. **GPT** - Assemble tout en un modèle\n",
    "\n",
    "## Pour aller plus loin:\n",
    "\n",
    "- Entraîner sur plus de données (Wikipedia, livres, etc.)\n",
    "- Augmenter la taille du modèle\n",
    "- Ajouter Flash Attention pour l'efficacité\n",
    "- Fine-tuning avec instruction following\n",
    "- RLHF (Reinforcement Learning from Human Feedback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
