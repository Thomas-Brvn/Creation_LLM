# Cr√©er un LLM from Scratch

Projet √©ducatif pour comprendre comment fonctionne un Large Language Model (LLM) de type GPT, √©tape par √©tape.

---

## Introduction

### C'est quoi un LLM ?

Un LLM (Large Language Model) est un mod√®le d'intelligence artificielle capable de comprendre et g√©n√©rer du texte. ChatGPT, Claude, LLaMA sont des exemples de LLMs.

### Comment √ßa fonctionne ?

Le principe est simple : **pr√©dire le mot suivant**.

```
Entr√©e:  "Le chat mange la"
Sortie:  "souris" (pr√©diction)
```

En r√©p√©tant cette pr√©diction, le mod√®le g√©n√®re du texte :

```
"Le chat" ‚Üí "mange"
"Le chat mange" ‚Üí "la"
"Le chat mange la" ‚Üí "souris"
...
```

### Le pipeline complet

```
Texte brut
    ‚Üì
1. TOKENIZATION  ‚Üí  Convertir texte en nombres
    ‚Üì
2. EMBEDDING     ‚Üí  Convertir nombres en vecteurs
    ‚Üì
3. ATTENTION     ‚Üí  Comprendre le contexte
    ‚Üì
4. G√âN√âRATION    ‚Üí  Pr√©dire le prochain token
```

---

## Partie 1 : Tokenization (BPE)

Un mod√®le ne comprend pas le texte, seulement des **nombres**. La tokenization convertit le texte en IDs :

```
"Bonjour le monde" ‚Üí [456, 12, 892] ‚Üí Le mod√®le
```

**BPE (Byte Pair Encoding)** fusionne les caract√®res fr√©quents pour cr√©er un vocabulaire efficace. Avantage : aucun mot n'est "inconnu", tout peut √™tre tokeniz√©.

<details>
<summary><strong>D√©tails complets sur BPE</strong></summary>

---

### Les 256 bytes de base

```
1 byte = 8 bits = 2‚Å∏ = 256 valeurs possibles (0 √† 255)
```

C'est la base de l'informatique. Chaque caract√®re a un code :

```
"A" = 65    "a" = 97    " " = 32
"√©" = 195 + 169 (2 bytes en UTF-8)
```

Tout texte peut √™tre repr√©sent√© en bytes ‚Üí BPE peut tokenizer **n'importe quel texte**.

---

### L'algorithme BPE

**Id√©e :** Fusionner les paires de caract√®res les plus fr√©quentes.

**Exemple** avec `"abab abab"` :

```
D√©part:  [a, b, a, b, ' ', a, b, a, b]  ‚Üí  9 tokens

Paire (a,b) appara√Æt 4 fois ‚Üí fusion en "ab"
         [ab, ab, ' ', ab, ab]          ‚Üí  5 tokens

Paire (ab,ab) appara√Æt 2 fois ‚Üí fusion en "abab"
         [abab, ' ', abab]              ‚Üí  3 tokens
```

**R√©sultat :** 9 tokens ‚Üí 3 tokens !

---

### vocab_size

```
vocab_size = 256 (bytes) + 4 (sp√©ciaux) + nombre de merges
```

| vocab_size | S√©quences | Mod√®le |
|------------|-----------|--------|
| Petit (1K) | Longues | L√©ger |
| Grand (50K) | Courtes | Lourd |

---

### Tokens sp√©ciaux

| Token | R√¥le |
|-------|------|
| `<pad>` | Remplissage |
| `<unk>` | Mot inconnu (jamais utilis√© avec BPE) |
| `<bos>` | D√©but de s√©quence |
| `<eos>` | Fin de s√©quence |

---

### BPE vs ancien syst√®me

```
Ancien:  "quinoa" ‚Üí <UNK>  (mot inconnu !)
BPE:     "quinoa" ‚Üí [qui][no][a]  (toujours d√©coupable)
```

</details>

### Questions de v√©rification

1. Pourquoi exactement 256 bytes de base ?
2. Un mot invent√© "xkzbrt" g√©n√®re-t-il une erreur avec BPE ?
3. Si j'augmente vocab_size, les s√©quences sont plus courtes ou plus longues ?

---

## Partie 2 : Embeddings

Les IDs de tokens sont juste des indices (456, 12, 892...). Le mod√®le a besoin de **vecteurs riches** pour capturer le sens. L'embedding convertit chaque ID en un vecteur de dimension `d_model` :

```
Token ID 456 ("chat") ‚Üí [0.2, -0.5, 0.8, ..., 0.1]  (384 dimensions)
```

Ces vecteurs sont **appris** pendant l'entra√Ænement : les mots similaires finissent proches dans l'espace vectoriel.

<details>
<summary><strong>üìñ Voir les d√©tails complets sur les Embeddings</strong></summary>

---

### La table d'embedding

C'est une matrice de taille `vocab_size √ó d_model` :

```
vocab_size = 8192 tokens
d_model = 384 dimensions

Table: 8192 √ó 384 = 3,145,728 param√®tres
```

Chaque ligne correspond √† un token :

```
ID 0   ‚Üí [0.1, 0.3, -0.2, ...]   (ligne 0)
ID 1   ‚Üí [0.5, -0.1, 0.7, ...]   (ligne 1)
...
ID 456 ‚Üí [0.2, -0.5, 0.8, ...]   (ligne 456 = "chat")
```

---

### Lookup (recherche)

L'embedding est juste une recherche dans la table :

```python
# Pseudo-code
embedding_table = matrix[vocab_size, d_model]

def embed(token_id):
    return embedding_table[token_id]  # Retourne la ligne
```

```
Entr√©e:  [456, 12, 892]  (3 token IDs)
Sortie:  [[...], [...], [...]]  (3 vecteurs de 384 dim)
         ‚Üí Tensor de shape (3, 384)
```

---

### Pourquoi d_model ?

`d_model` = dimension des vecteurs dans tout le mod√®le.

| d_model | Capacit√© | Param√®tres |
|---------|----------|------------|
| 128 | Faible | L√©ger |
| 384 | Moyenne | ~10M params |
| 768 | Haute | ~100M params |
| 4096 | Tr√®s haute | GPT-3 scale |

Plus `d_model` est grand, plus le mod√®le peut encoder d'information par token.

---

### Propri√©t√© : mots similaires = vecteurs proches

Apr√®s entra√Ænement, les embeddings capturent le sens :

```
distance("roi", "reine") < distance("roi", "voiture")
distance("chat", "chien") < distance("chat", "avion")
```

On peut m√™me faire de l'arithm√©tique :

```
embedding("roi") - embedding("homme") + embedding("femme") ‚âà embedding("reine")
```

---

### En r√©sum√©

```
Token IDs        ‚Üí  Embedding Table  ‚Üí  Vecteurs
[456, 12, 892]   ‚Üí  lookup           ‚Üí  (3, 384)
```

</details>

### Questions de v√©rification

1. Quelle est la taille de la table d'embedding si vocab_size=4096 et d_model=256 ?
2. L'embedding est-il appris ou fix√© √† l'avance ?
3. Pourquoi les mots similaires ont-ils des vecteurs proches ?

---

## Partie 3 : Attention (concept)

Le mot "**il**" dans "Le chat dort car **il** est fatigu√©" fait r√©f√©rence √† "chat". Comment le mod√®le le sait-il ? Gr√¢ce √† l'**attention** : chaque token regarde les autres tokens pour comprendre le contexte.

```
"il" regarde ‚Üí ["Le", "chat", "dort", "car"] ‚Üí comprend que "il" = "chat"
```

L'attention calcule un **score de pertinence** entre chaque paire de tokens, puis fait une moyenne pond√©r√©e.

<details>
<summary><strong>üìñ Voir les d√©tails complets sur l'Attention</strong></summary>

---

### Pourquoi l'attention ?

Sans contexte, les mots sont ambigus :

```
"La souris mange le fromage"  ‚Üí souris = animal
"La souris ne marche plus"    ‚Üí souris = p√©riph√©rique
```

L'attention permet au mod√®le de **regarder les autres mots** pour lever l'ambigu√Øt√©.

---

### Self-Attention

"Self" car les tokens d'une m√™me s√©quence s'observent entre eux :

```
S√©quence: ["Le", "chat", "mange"]

"Le"    regarde: ["Le", "chat", "mange"]
"chat"  regarde: ["Le", "chat", "mange"]
"mange" regarde: ["Le", "chat", "mange"]
```

Chaque token calcule √† quel point les autres tokens sont **pertinents** pour lui.

---

### Intuition : la f√™te

Imagine une f√™te avec 4 personnes. Tu veux savoir √† qui parler :

1. Tu regardes chaque personne (calcul des scores)
2. Tu d√©cides qui est int√©ressant pour toi (scores de pertinence)
3. Tu √©coutes plus ceux qui t'int√©ressent (moyenne pond√©r√©e)

```
Toi ‚Üí [Alice: 0.5, Bob: 0.3, Claire: 0.2]
        ‚Üì
Tu absorbes 50% d'Alice, 30% de Bob, 20% de Claire
```

C'est exactement ce que fait l'attention avec les tokens.

---

### Masquage causal (GPT)

Dans un LLM comme GPT, un token ne peut voir que les tokens **pr√©c√©dents** (pas le futur) :

```
S√©quence: ["Le", "chat", "mange", "la", "souris"]

"Le"     voit: ["Le"]
"chat"   voit: ["Le", "chat"]
"mange"  voit: ["Le", "chat", "mange"]
"la"     voit: ["Le", "chat", "mange", "la"]
"souris" voit: ["Le", "chat", "mange", "la", "souris"]
```

Pourquoi ? Sinon le mod√®le "tricherait" en regardant la r√©ponse pendant l'entra√Ænement.

---

### Complexit√© O(n¬≤)

Chaque token regarde **tous** les autres tokens :

```
n tokens ‚Üí n √ó n = n¬≤ comparaisons

 64 tokens  ‚Üí    4,096 comparaisons
256 tokens  ‚Üí   65,536 comparaisons
1024 tokens ‚Üí 1,048,576 comparaisons
```

C'est pourquoi les LLMs ont une limite de contexte (`max_seq_len`).

---

### En r√©sum√©

```
Embeddings (n, d_model)
         ‚Üì
    Self-Attention  ‚Üí  Chaque token regarde les autres
         ‚Üì
Contexte enrichi (n, d_model)
```

</details>

### Questions de v√©rification

1. Pourquoi "il" a besoin de regarder les autres mots ?
2. Dans GPT, le 3√®me token peut-il voir le 5√®me token ?
3. Pourquoi la complexit√© est O(n¬≤) ?

---

## Partie 4 : Attention (calculs)

L'attention utilise trois vecteurs par token : **Query** (ce que je cherche), **Key** (ce que je contiens), **Value** (l'info que je donne). La formule :

```
Attention(Q, K, V) = softmax(Q √ó K·µÄ / ‚àöd_k) √ó V
```

En gros : on calcule la similarit√© entre Q et K, on normalise avec softmax, puis on fait une moyenne pond√©r√©e des V.

<details>
<summary><strong>üìñ Voir les d√©tails complets sur Q, K, V</strong></summary>

---

### Query, Key, Value - Intuition

Imagine une biblioth√®que :

| Concept | Analogie | R√¥le |
|---------|----------|------|
| **Query (Q)** | Ta question | "Je cherche des infos sur les chats" |
| **Key (K)** | Titre du livre | "Animaux domestiques", "Cuisine", ... |
| **Value (V)** | Contenu du livre | L'information utile |

Tu compares ta **question** (Q) avec les **titres** (K) pour trouver les livres pertinents, puis tu lis leur **contenu** (V).

---

### Comment obtenir Q, K, V ?

Chaque token a un embedding. On le projette avec 3 matrices apprises :

```
embedding (d_model) ‚Üí W_Q ‚Üí Query  (d_k)
embedding (d_model) ‚Üí W_K ‚Üí Key    (d_k)
embedding (d_model) ‚Üí W_V ‚Üí Value  (d_v)
```

```python
Q = embedding @ W_Q  # (n, d_model) @ (d_model, d_k) ‚Üí (n, d_k)
K = embedding @ W_K  # (n, d_model) @ (d_model, d_k) ‚Üí (n, d_k)
V = embedding @ W_V  # (n, d_model) @ (d_model, d_v) ‚Üí (n, d_v)
```

---

### √âtape 1 : Scores d'attention

On calcule la similarit√© entre chaque Q et chaque K :

```
scores = Q √ó K·µÄ
```

```
Q: (n, d_k)
K: (n, d_k) ‚Üí K·µÄ: (d_k, n)

scores = Q @ K·µÄ = (n, d_k) @ (d_k, n) = (n, n)
```

R√©sultat : une matrice (n √ó n) o√π `scores[i][j]` = similarit√© entre token i et token j.

---

### √âtape 2 : Mise √† l'√©chelle

On divise par ‚àöd_k pour stabiliser les gradients :

```
scores = scores / ‚àöd_k
```

Sans √ßa, les scores deviennent trop grands ‚Üí softmax sature ‚Üí gradients nuls.

---

### √âtape 3 : Masquage causal (optionnel)

Pour GPT, on masque le futur avec -‚àû :

```
scores (avant masque):       scores (apr√®s masque):
[[0.5, 0.3, 0.2]             [[0.5,  -‚àû,  -‚àû]
 [0.4, 0.6, 0.1]       ‚Üí      [0.4, 0.6,  -‚àû]
 [0.2, 0.3, 0.5]]             [0.2, 0.3, 0.5]]
```

---

### √âtape 4 : Softmax

On convertit les scores en probabilit√©s (somme = 1 par ligne) :

```
weights = softmax(scores)
```

```
scores: [2.0, 1.0, -‚àû]  ‚Üí  weights: [0.73, 0.27, 0.00]
```

---

### √âtape 5 : Moyenne pond√©r√©e des Values

```
output = weights √ó V
```

```
weights: (n, n)
V: (n, d_v)

output = weights @ V = (n, n) @ (n, d_v) = (n, d_v)
```

Chaque token obtient un m√©lange des Values des autres tokens.

---

### Formule compl√®te

```
Attention(Q, K, V) = softmax(Q √ó K·µÄ / ‚àöd_k) √ó V
```

```
Entr√©e:  embeddings (n, d_model)
         ‚Üì
      Q, K, V via projections
         ‚Üì
      scores = Q @ K·µÄ / ‚àöd_k     ‚Üí (n, n)
         ‚Üì
      weights = softmax(scores)  ‚Üí (n, n)
         ‚Üì
      output = weights @ V       ‚Üí (n, d_v)
```

---

### Exemple num√©rique simplifi√©

3 tokens, d_k = 2 :

```
Q = [[1, 0],    K = [[1, 0],    V = [[1, 2],
     [0, 1],         [0, 1],         [3, 4],
     [1, 1]]         [1, 1]]         [5, 6]]

scores = Q @ K·µÄ = [[1, 0, 1],
                   [0, 1, 1],
                   [1, 1, 2]]

scores / ‚àö2 = [[0.71, 0.00, 0.71],
               [0.00, 0.71, 0.71],
               [0.71, 0.71, 1.41]]

weights = softmax(...) ‚âà [[0.39, 0.22, 0.39],
                          [0.22, 0.39, 0.39],
                          [0.26, 0.26, 0.48]]

output = weights @ V  (m√©lange pond√©r√©)
```

</details>

### Questions de v√©rification

1. √Ä quoi sert la division par ‚àöd_k ?
2. Quelle est la shape de la matrice de scores pour 10 tokens ?
3. Pourquoi met-on -‚àû (et pas 0) pour masquer le futur ?

---

## Prochaines parties
- **Partie 5** : Multi-Head Attention
- **Partie 6** : Positional Encoding (RoPE)
- **Partie 7** : Feed-Forward, RMSNorm, r√©siduel
- **Partie 8** : Architecture GPT compl√®te
- **Partie 9** : Entra√Ænement
- **Partie 10** : G√©n√©ration de texte

---

## Structure du projet

```
Creation_LLM/
‚îú‚îÄ‚îÄ README.md              ‚Üê Ce fichier
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py       ‚Üê Impl√©mentation BPE
‚îÇ   ‚îú‚îÄ‚îÄ model.py           ‚Üê Architecture Transformer
‚îÇ   ‚îú‚îÄ‚îÄ train.py           ‚Üê Script d'entra√Ænement
‚îÇ   ‚îî‚îÄ‚îÄ generate.py        ‚Üê Script de g√©n√©ration
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ CONCEPTS_LLM.md    ‚Üê R√©capitulatif technique
‚îî‚îÄ‚îÄ data/                  ‚Üê Donn√©es d'entra√Ænement
```

---

## Ressources

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Andrej Karpathy - Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [Attention Is All You Need (paper)](https://arxiv.org/abs/1706.03762)

# Pair programming contribution
