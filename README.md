# CrÃ©er un LLM from Scratch

Projet Ã©ducatif pour comprendre comment fonctionne un Large Language Model (LLM) de type GPT, Ã©tape par Ã©tape.

---

## Introduction

### C'est quoi un LLM ?

Un LLM (Large Language Model) est un modÃ¨le d'intelligence artificielle capable de comprendre et gÃ©nÃ©rer du texte. ChatGPT, Claude, LLaMA sont des exemples de LLMs.

### Comment Ã§a fonctionne ?

Le principe est simple : **prÃ©dire le mot suivant**.

```
EntrÃ©e:  "Le chat mange la"
Sortie:  "souris" (prÃ©diction)
```

En rÃ©pÃ©tant cette prÃ©diction, le modÃ¨le gÃ©nÃ¨re du texte :

```
"Le chat" â†’ "mange"
"Le chat mange" â†’ "la"
"Le chat mange la" â†’ "souris"
...
```

### Le pipeline complet

```
Texte brut
    â†“
1. TOKENIZATION  â†’  Convertir texte en nombres
    â†“
2. EMBEDDING     â†’  Convertir nombres en vecteurs
    â†“
3. ATTENTION     â†’  Comprendre le contexte
    â†“
4. GÃ‰NÃ‰RATION    â†’  PrÃ©dire le prochain token
```

---

## Partie 1 : Tokenization (BPE)

Un modÃ¨le ne comprend pas le texte, seulement des **nombres**. La tokenization convertit le texte en IDs :

```
"Bonjour le monde" â†’ [456, 12, 892] â†’ Le modÃ¨le
```

**BPE (Byte Pair Encoding)** fusionne les caractÃ¨res frÃ©quents pour crÃ©er un vocabulaire efficace. Avantage : aucun mot n'est "inconnu", tout peut Ãªtre tokenizÃ©.

<details>
<summary><strong>DÃ©tails complets sur BPE</strong></summary>

---

### Les 256 bytes de base

```
1 byte = 8 bits = 2â¸ = 256 valeurs possibles (0 Ã  255)
```

C'est la base de l'informatique. Chaque caractÃ¨re a un code :

```
"A" = 65    "a" = 97    " " = 32
"Ã©" = 195 + 169 (2 bytes en UTF-8)
```

Tout texte peut Ãªtre reprÃ©sentÃ© en bytes â†’ BPE peut tokenizer **n'importe quel texte**.

---

### L'algorithme BPE

**IdÃ©e :** Fusionner les paires de caractÃ¨res les plus frÃ©quentes.

**Exemple** avec `"abab abab"` :

```
DÃ©part:  [a, b, a, b, ' ', a, b, a, b]  â†’  9 tokens

Paire (a,b) apparaÃ®t 4 fois â†’ fusion en "ab"
         [ab, ab, ' ', ab, ab]          â†’  5 tokens

Paire (ab,ab) apparaÃ®t 2 fois â†’ fusion en "abab"
         [abab, ' ', abab]              â†’  3 tokens
```

**RÃ©sultat :** 9 tokens â†’ 3 tokens !

---

### vocab_size

```
vocab_size = 256 (bytes) + 4 (spÃ©ciaux) + nombre de merges
```

| vocab_size | SÃ©quences | ModÃ¨le |
|------------|-----------|--------|
| Petit (1K) | Longues | LÃ©ger |
| Grand (50K) | Courtes | Lourd |

---

### Tokens spÃ©ciaux

| Token | RÃ´le |
|-------|------|
| `<pad>` | Remplissage |
| `<unk>` | Mot inconnu (jamais utilisÃ© avec BPE) |
| `<bos>` | DÃ©but de sÃ©quence |
| `<eos>` | Fin de sÃ©quence |

---

### BPE vs ancien systÃ¨me

```
Ancien:  "quinoa" â†’ <UNK>  (mot inconnu !)
BPE:     "quinoa" â†’ [qui][no][a]  (toujours dÃ©coupable)
```

</details>

### Questions de vÃ©rification

1. Pourquoi exactement 256 bytes de base ?
2. Un mot inventÃ© "xkzbrt" gÃ©nÃ¨re-t-il une erreur avec BPE ?
3. Si j'augmente vocab_size, les sÃ©quences sont plus courtes ou plus longues ?

---

## Partie 2 : Embeddings

Les IDs de tokens sont juste des indices (456, 12, 892...). Le modÃ¨le a besoin de **vecteurs riches** pour capturer le sens. L'embedding convertit chaque ID en un vecteur de dimension `d_model` :

```
Token ID 456 ("chat") â†’ [0.2, -0.5, 0.8, ..., 0.1]  (384 dimensions)
```

Ces vecteurs sont **appris** pendant l'entraÃ®nement : les mots similaires finissent proches dans l'espace vectoriel.

<details>
<summary><strong>ğŸ“– Voir les dÃ©tails complets sur les Embeddings</strong></summary>

---

### La table d'embedding

C'est une matrice de taille `vocab_size Ã— d_model` :

```
vocab_size = 8192 tokens
d_model = 384 dimensions

Table: 8192 Ã— 384 = 3,145,728 paramÃ¨tres
```

Chaque ligne correspond Ã  un token :

```
ID 0   â†’ [0.1, 0.3, -0.2, ...]   (ligne 0)
ID 1   â†’ [0.5, -0.1, 0.7, ...]   (ligne 1)
...
ID 456 â†’ [0.2, -0.5, 0.8, ...]   (ligne 456 = "chat")
```

---

### Lookup (recherche)

L'embedding est juste une recherche dans la table :

```python
# Pseudo-code
embedding_table = matrix[vocab_size, d_model]

def embed(token_id):
    return embedding_table[token_id]  # Retourne la ligne
```

```
EntrÃ©e:  [456, 12, 892]  (3 token IDs)
Sortie:  [[...], [...], [...]]  (3 vecteurs de 384 dim)
         â†’ Tensor de shape (3, 384)
```

---

### Pourquoi d_model ?

`d_model` = dimension des vecteurs dans tout le modÃ¨le.

| d_model | CapacitÃ© | ParamÃ¨tres |
|---------|----------|------------|
| 128 | Faible | LÃ©ger |
| 384 | Moyenne | ~10M params |
| 768 | Haute | ~100M params |
| 4096 | TrÃ¨s haute | GPT-3 scale |

Plus `d_model` est grand, plus le modÃ¨le peut encoder d'information par token.

---

### PropriÃ©tÃ© : mots similaires = vecteurs proches

AprÃ¨s entraÃ®nement, les embeddings capturent le sens :

```
distance("roi", "reine") < distance("roi", "voiture")
distance("chat", "chien") < distance("chat", "avion")
```

On peut mÃªme faire de l'arithmÃ©tique :

```
embedding("roi") - embedding("homme") + embedding("femme") â‰ˆ embedding("reine")
```

---

### En rÃ©sumÃ©

```
Token IDs        â†’  Embedding Table  â†’  Vecteurs
[456, 12, 892]   â†’  lookup           â†’  (3, 384)
```

</details>

### Questions de vÃ©rification

1. Quelle est la taille de la table d'embedding si vocab_size=4096 et d_model=256 ?
2. L'embedding est-il appris ou fixÃ© Ã  l'avance ?
3. Pourquoi les mots similaires ont-ils des vecteurs proches ?

---

## Partie 3 : Attention (concept)

Le mot "**il**" dans "Le chat dort car **il** est fatiguÃ©" fait rÃ©fÃ©rence Ã  "chat". Comment le modÃ¨le le sait-il ? GrÃ¢ce Ã  l'**attention** : chaque token regarde les autres tokens pour comprendre le contexte.

```
"il" regarde â†’ ["Le", "chat", "dort", "car"] â†’ comprend que "il" = "chat"
```

L'attention calcule un **score de pertinence** entre chaque paire de tokens, puis fait une moyenne pondÃ©rÃ©e.

<details>
<summary><strong>ğŸ“– Voir les dÃ©tails complets sur l'Attention</strong></summary>

---

### Pourquoi l'attention ?

Sans contexte, les mots sont ambigus :

```
"La souris mange le fromage"  â†’ souris = animal
"La souris ne marche plus"    â†’ souris = pÃ©riphÃ©rique
```

L'attention permet au modÃ¨le de **regarder les autres mots** pour lever l'ambiguÃ¯tÃ©.

---

### Self-Attention

"Self" car les tokens d'une mÃªme sÃ©quence s'observent entre eux :

```
SÃ©quence: ["Le", "chat", "mange"]

"Le"    regarde: ["Le", "chat", "mange"]
"chat"  regarde: ["Le", "chat", "mange"]
"mange" regarde: ["Le", "chat", "mange"]
```

Chaque token calcule Ã  quel point les autres tokens sont **pertinents** pour lui.

---

### Intuition : la fÃªte

Imagine une fÃªte avec 4 personnes. Tu veux savoir Ã  qui parler :

1. Tu regardes chaque personne (calcul des scores)
2. Tu dÃ©cides qui est intÃ©ressant pour toi (scores de pertinence)
3. Tu Ã©coutes plus ceux qui t'intÃ©ressent (moyenne pondÃ©rÃ©e)

```
Toi â†’ [Alice: 0.5, Bob: 0.3, Claire: 0.2]
        â†“
Tu absorbes 50% d'Alice, 30% de Bob, 20% de Claire
```

C'est exactement ce que fait l'attention avec les tokens.

---

### Masquage causal (GPT)

Dans un LLM comme GPT, un token ne peut voir que les tokens **prÃ©cÃ©dents** (pas le futur) :

```
SÃ©quence: ["Le", "chat", "mange", "la", "souris"]

"Le"     voit: ["Le"]
"chat"   voit: ["Le", "chat"]
"mange"  voit: ["Le", "chat", "mange"]
"la"     voit: ["Le", "chat", "mange", "la"]
"souris" voit: ["Le", "chat", "mange", "la", "souris"]
```

Pourquoi ? Sinon le modÃ¨le "tricherait" en regardant la rÃ©ponse pendant l'entraÃ®nement.

---

### ComplexitÃ© O(nÂ²)

Chaque token regarde **tous** les autres tokens :

```
n tokens â†’ n Ã— n = nÂ² comparaisons

 64 tokens  â†’    4,096 comparaisons
256 tokens  â†’   65,536 comparaisons
1024 tokens â†’ 1,048,576 comparaisons
```

C'est pourquoi les LLMs ont une limite de contexte (`max_seq_len`).

---

### En rÃ©sumÃ©

```
Embeddings (n, d_model)
         â†“
    Self-Attention  â†’  Chaque token regarde les autres
         â†“
Contexte enrichi (n, d_model)
```

</details>

### Questions de vÃ©rification

1. Pourquoi "il" a besoin de regarder les autres mots ?
2. Dans GPT, le 3Ã¨me token peut-il voir le 5Ã¨me token ?
3. Pourquoi la complexitÃ© est O(nÂ²) ?

---

## Partie 4 : Attention (calculs)

L'attention utilise trois vecteurs par token : **Query** (ce que je cherche), **Key** (ce que je contiens), **Value** (l'info que je donne). La formule :

```
Attention(Q, K, V) = softmax(Q Ã— Káµ€ / âˆšd_k) Ã— V
```

En gros : on calcule la similaritÃ© entre Q et K, on normalise avec softmax, puis on fait une moyenne pondÃ©rÃ©e des V.

<details>
<summary><strong>ğŸ“– Voir les dÃ©tails complets sur Q, K, V</strong></summary>

---

### Query, Key, Value - Intuition

Imagine une bibliothÃ¨que :

| Concept | Analogie | RÃ´le |
|---------|----------|------|
| **Query (Q)** | Ta question | "Je cherche des infos sur les chats" |
| **Key (K)** | Titre du livre | "Animaux domestiques", "Cuisine", ... |
| **Value (V)** | Contenu du livre | L'information utile |

Tu compares ta **question** (Q) avec les **titres** (K) pour trouver les livres pertinents, puis tu lis leur **contenu** (V).

---

### Comment obtenir Q, K, V ?

Chaque token a un embedding. On le projette avec 3 matrices apprises :

```
embedding (d_model) â†’ W_Q â†’ Query  (d_k)
embedding (d_model) â†’ W_K â†’ Key    (d_k)
embedding (d_model) â†’ W_V â†’ Value  (d_v)
```

```python
Q = embedding @ W_Q  # (n, d_model) @ (d_model, d_k) â†’ (n, d_k)
K = embedding @ W_K  # (n, d_model) @ (d_model, d_k) â†’ (n, d_k)
V = embedding @ W_V  # (n, d_model) @ (d_model, d_v) â†’ (n, d_v)
```

---

### Ã‰tape 1 : Scores d'attention

On calcule la similaritÃ© entre chaque Q et chaque K :

```
scores = Q Ã— Káµ€
```

```
Q: (n, d_k)
K: (n, d_k) â†’ Káµ€: (d_k, n)

scores = Q @ Káµ€ = (n, d_k) @ (d_k, n) = (n, n)
```

RÃ©sultat : une matrice (n Ã— n) oÃ¹ `scores[i][j]` = similaritÃ© entre token i et token j.

---

### Ã‰tape 2 : Mise Ã  l'Ã©chelle

On divise par âˆšd_k pour stabiliser les gradients :

```
scores = scores / âˆšd_k
```

Sans Ã§a, les scores deviennent trop grands â†’ softmax sature â†’ gradients nuls.

---

### Ã‰tape 3 : Masquage causal (optionnel)

Pour GPT, on masque le futur avec -âˆ :

```
scores (avant masque):       scores (aprÃ¨s masque):
[[0.5, 0.3, 0.2]             [[0.5,  -âˆ,  -âˆ]
 [0.4, 0.6, 0.1]       â†’      [0.4, 0.6,  -âˆ]
 [0.2, 0.3, 0.5]]             [0.2, 0.3, 0.5]]
```

---

### Ã‰tape 4 : Softmax

On convertit les scores en probabilitÃ©s (somme = 1 par ligne) :

```
weights = softmax(scores)
```

```
scores: [2.0, 1.0, -âˆ]  â†’  weights: [0.73, 0.27, 0.00]
```

---

### Ã‰tape 5 : Moyenne pondÃ©rÃ©e des Values

```
output = weights Ã— V
```

```
weights: (n, n)
V: (n, d_v)

output = weights @ V = (n, n) @ (n, d_v) = (n, d_v)
```

Chaque token obtient un mÃ©lange des Values des autres tokens.

---

### Formule complÃ¨te

```
Attention(Q, K, V) = softmax(Q Ã— Káµ€ / âˆšd_k) Ã— V
```

```
EntrÃ©e:  embeddings (n, d_model)
         â†“
      Q, K, V via projections
         â†“
      scores = Q @ Káµ€ / âˆšd_k     â†’ (n, n)
         â†“
      weights = softmax(scores)  â†’ (n, n)
         â†“
      output = weights @ V       â†’ (n, d_v)
```

---

### Exemple numÃ©rique simplifiÃ©

3 tokens, d_k = 2 :

```
Q = [[1, 0],    K = [[1, 0],    V = [[1, 2],
     [0, 1],         [0, 1],         [3, 4],
     [1, 1]]         [1, 1]]         [5, 6]]

scores = Q @ Káµ€ = [[1, 0, 1],
                   [0, 1, 1],
                   [1, 1, 2]]

scores / âˆš2 = [[0.71, 0.00, 0.71],
               [0.00, 0.71, 0.71],
               [0.71, 0.71, 1.41]]

weights = softmax(...) â‰ˆ [[0.39, 0.22, 0.39],
                          [0.22, 0.39, 0.39],
                          [0.26, 0.26, 0.48]]

output = weights @ V  (mÃ©lange pondÃ©rÃ©)
```

</details>

### Questions de vÃ©rification

1. Ã€ quoi sert la division par âˆšd_k ?
2. Quelle est la shape de la matrice de scores pour 10 tokens ?
3. Pourquoi met-on -âˆ (et pas 0) pour masquer le futur ?

---

## Partie 5 : Multi-Head Attention

Une seule attention capture un seul "point de vue". Avec **plusieurs tÃªtes** en parallÃ¨le, le modÃ¨le peut capturer diffÃ©rents types de relations :

```
TÃªte 1 : relations syntaxiques (sujet â†’ verbe)
TÃªte 2 : relations sÃ©mantiques (chat â†’ animal)
TÃªte 3 : proximitÃ© (mots proches)
...
```

On divise `d_model` entre les tÃªtes : avec 384 dimensions et 6 tÃªtes, chaque tÃªte travaille sur 64 dimensions.

<details>
<summary><strong>ğŸ“– Voir les dÃ©tails complets sur Multi-Head Attention</strong></summary>

---

### Pourquoi plusieurs tÃªtes ?

Une seule attention = un seul type de relation. Mais le langage est complexe :

```
"Le chat que j'ai adoptÃ© mange"

- Relation syntaxique : "mange" â†’ "chat" (sujet)
- Relation rÃ©fÃ©rentielle : "j'" â†’ locuteur
- Relation temporelle : "ai adoptÃ©" â†’ passÃ©
```

Chaque tÃªte peut se spÃ©cialiser sur un aspect diffÃ©rent.

---

### Comment Ã§a marche ?

On fait **n_heads** attentions en parallÃ¨le, chacune sur une portion de `d_model` :

```
d_model = 384
n_heads = 6
d_k = d_model / n_heads = 64  (par tÃªte)
```

```
                    Embedding (n, 384)
                           â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                  â†“                  â†“
    Head 1 (64)        Head 2 (64)   ...  Head 6 (64)
        â†“                  â†“                  â†“
    Attention          Attention         Attention
        â†“                  â†“                  â†“
    Output (64)        Output (64)  ...  Output (64)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
                    Concat (n, 384)
                           â†“
                    Projection W_O
                           â†“
                    Output (n, 384)
```

---

### Les projections

Chaque tÃªte a ses propres matrices W_Q, W_K, W_V :

```python
# Pour chaque tÃªte i
Q_i = X @ W_Q_i  # (n, d_model) @ (d_model, d_k) â†’ (n, d_k)
K_i = X @ W_K_i
V_i = X @ W_V_i

head_i = Attention(Q_i, K_i, V_i)  # (n, d_k)
```

En pratique, on fait tout en une seule opÃ©ration matricielle pour l'efficacitÃ©.

---

### Concat + Projection finale

```python
# ConcatÃ©ner toutes les tÃªtes
concat = [head_1, head_2, ..., head_n]  # (n, n_heads * d_k) = (n, d_model)

# Projection de sortie
output = concat @ W_O  # (n, d_model) @ (d_model, d_model) â†’ (n, d_model)
```

W_O permet de mÃ©langer les informations des diffÃ©rentes tÃªtes.

---

### Exemple concret

```
ModÃ¨le : d_model=384, n_heads=6

EntrÃ©e: (batch=32, seq=128, d_model=384)

Pour chaque tÃªte (6 fois en parallÃ¨le):
  Q, K, V: (32, 128, 64)
  scores:  (32, 128, 128)
  output:  (32, 128, 64)

Concat: (32, 128, 384)
AprÃ¨s W_O: (32, 128, 384)
```

---

### Visualisation des tÃªtes

AprÃ¨s entraÃ®nement, on peut visualiser ce que chaque tÃªte "regarde" :

```
Phrase: "Le chat dort sur le canapÃ©"

TÃªte 1: "dort" regarde fortement "chat"  (sujet-verbe)
TÃªte 2: "le" regarde "canapÃ©"            (dÃ©terminant-nom)
TÃªte 3: tous regardent les voisins       (localitÃ©)
```

---

### ParamÃ¨tres

```
Par tÃªte:
  W_Q: d_model Ã— d_k
  W_K: d_model Ã— d_k
  W_V: d_model Ã— d_k

Total pour n_heads:
  3 Ã— n_heads Ã— d_model Ã— d_k = 3 Ã— d_modelÂ²

Plus W_O:
  d_model Ã— d_model

Total Multi-Head Attention â‰ˆ 4 Ã— d_modelÂ²
```

</details>

### Questions de vÃ©rification

1. Si d_model=512 et n_heads=8, quelle est la dimension par tÃªte ?
2. Pourquoi utiliser plusieurs petites tÃªtes plutÃ´t qu'une grande ?
3. Ã€ quoi sert la matrice W_O ?

---

## Partie 6 : Positional Encoding (RoPE)

L'attention ne connaÃ®t **pas l'ordre des mots**. "Le chat mange la souris" et "La souris mange le chat" produiraient le mÃªme rÃ©sultat sans encodage positionnel. **RoPE** (Rotary Position Embedding) injecte la position de chaque token en **tournant** ses vecteurs Q et K dans l'espace.

```
Position 0 â†’ rotation de 0Â°
Position 1 â†’ rotation de Î¸Â°
Position 2 â†’ rotation de 2Î¸Â°
...
```

<details>
<summary><strong>ğŸ“– Voir les dÃ©tails complets sur RoPE</strong></summary>

---

### Le problÃ¨me

L'attention calcule Q Ã— Káµ€. C'est un produit scalaire, qui est **invariant Ã  l'ordre** :

```
Tokens: ["chat", "mange"]  â†’  score = Q_chat Â· K_mange
Tokens: ["mange", "chat"]  â†’  score = Q_chat Â· K_mange  (identique !)
```

Le modÃ¨le ne sait pas qui vient avant qui.

---

### Anciennes approches

**Positional Encoding sinusoÃ¯dal** (Transformer original) :

```
On additionne un vecteur de position Ã  l'embedding :

embedding_final = embedding + position_vector
```

ProblÃ¨me : la position est "mÃ©langÃ©e" avec le sens du mot.

**Positional Embedding appris** (GPT-2) :

```
Table de positions apprise : (max_seq_len, d_model)
embedding_final = embedding + position_embedding[pos]
```

ProblÃ¨me : limitÃ© Ã  max_seq_len positions vues Ã  l'entraÃ®nement.

---

### RoPE : l'idÃ©e

Au lieu d'**ajouter** la position, on **tourne** les vecteurs Q et K.

L'idÃ©e clÃ© : deux tokens Ã  la position i et j auront un score d'attention qui dÃ©pend uniquement de leur **distance relative** (j - i), pas de leur position absolue.

---

### Comment Ã§a marche ?

On prend les dimensions de Q et K **par paires** et on applique une rotation 2D :

```
Dimensions [0,1] : rotation de pos Ã— Î¸â‚
Dimensions [2,3] : rotation de pos Ã— Î¸â‚‚
Dimensions [4,5] : rotation de pos Ã— Î¸â‚ƒ
...
```

Chaque paire tourne Ã  une frÃ©quence diffÃ©rente :

```
Î¸_i = 1 / (10000^(2i/d_k))

Î¸â‚ = 1/10000^0     = 1.0       (haute frÃ©quence)
Î¸â‚‚ = 1/10000^0.031 = 0.90      (...)
...
Î¸â‚ƒâ‚‚ = 1/10000^1    = 0.0001    (basse frÃ©quence)
```

---

### Rotation 2D

Pour une paire de dimensions (qâ‚€, qâ‚) Ã  la position pos :

```
qâ‚€' = qâ‚€ Ã— cos(pos Ã— Î¸) - qâ‚ Ã— sin(pos Ã— Î¸)
qâ‚' = qâ‚€ Ã— sin(pos Ã— Î¸) + qâ‚ Ã— cos(pos Ã— Î¸)
```

C'est une simple rotation dans le plan.

---

### Pourquoi Ã§a encode la distance relative ?

Quand on calcule Q_i Â· K_j aprÃ¨s rotation :

```
score(i, j) = f(q, k, i-j)
```

Le score ne dÃ©pend que de la **diffÃ©rence** (i-j), pas des positions absolues. Le modÃ¨le comprend naturellement que :
- "chat" est 2 positions avant "mange"
- Peu importe que ce soit aux positions (0,2) ou (5,7)

---

### Avantages de RoPE

| PropriÃ©tÃ© | RoPE | SinusoÃ¯dal | Appris |
|-----------|------|------------|--------|
| Distance relative | Oui | Non | Non |
| Extrapolation (seq plus longues) | Bonne | Moyenne | Mauvaise |
| ParamÃ¨tres supplÃ©mentaires | 0 | 0 | max_seq Ã— d |
| UtilisÃ© par | LLaMA, Mistral | Transformer orig. | GPT-2 |

---

### En rÃ©sumÃ©

```
Q, K (n, d_k)
      â†“
  Rotation par position (RoPE)
      â†“
Q_rot, K_rot (n, d_k)
      â†“
  Attention classique (Q_rot Ã— K_rotáµ€ / âˆšd_k)
```

</details>

### Questions de vÃ©rification

1. Pourquoi l'attention seule ne connaÃ®t pas l'ordre des mots ?
2. Quelle est la diffÃ©rence entre ajouter la position et tourner les vecteurs ?
3. Pourquoi la distance relative est prÃ©fÃ©rable Ã  la position absolue ?

---

## Partie 7 : Feed-Forward, RMSNorm, connexions rÃ©siduelles

AprÃ¨s l'attention, chaque token passe dans un **rÃ©seau Feed-Forward** qui transforme l'information individuellement. **RMSNorm** stabilise les valeurs, et les **connexions rÃ©siduelles** (x + f(x)) permettent au gradient de circuler mÃªme dans un rÃ©seau trÃ¨s profond.

```
x â†’ RMSNorm â†’ Attention â†’ + x â†’ RMSNorm â†’ Feed-Forward â†’ + x
                          â†‘ rÃ©siduel                      â†‘ rÃ©siduel
```

<details>
<summary><strong>ğŸ“– Voir les dÃ©tails complets sur FFN, RMSNorm et rÃ©siduel</strong></summary>

---

### Feed-Forward Network (FFN)

AprÃ¨s l'attention (qui mÃ©lange les tokens), le FFN traite **chaque token indÃ©pendamment** :

```python
def feed_forward(x):          # x: (n, d_model)
    hidden = x @ W1 + b1      # (n, d_model) â†’ (n, d_ff)
    hidden = activation(hidden)
    output = hidden @ W2 + b2  # (n, d_ff) â†’ (n, d_model)
    return output
```

---

### Dimension cachÃ©e d_ff

Le FFN projette d'abord vers un espace plus grand, puis revient :

```
d_model (384) â†’ d_ff (1024) â†’ d_model (384)
```

Typiquement : `d_ff â‰ˆ 2.7 Ã— d_model` (avec SwiGLU).

Pourquoi ? L'espace Ã©largi permet des transformations plus riches.

---

### SwiGLU (activation moderne)

Les anciens Transformers utilisaient ReLU. Les modÃ¨les modernes (LLaMA, Mistral) utilisent **SwiGLU** :

```python
# ReLU classique
hidden = ReLU(x @ W1)

# SwiGLU (plus performant)
gate = sigmoid(x @ W_gate) * (x @ W_gate)  # "porte"
hidden = gate * (x @ W1)
```

SwiGLU utilise une matrice supplÃ©mentaire (W_gate) mais donne de meilleurs rÃ©sultats.

```
ParamÃ¨tres FFN:
  Classique: 2 Ã— d_model Ã— d_ff
  SwiGLU:    3 Ã— d_model Ã— d_ff (W1, W2, W_gate)
```

---

### RMSNorm

Normalise les vecteurs pour stabiliser l'entraÃ®nement :

```python
def rmsnorm(x):
    rms = sqrt(mean(xÂ²))  # Racine de la moyenne des carrÃ©s
    return (x / rms) * gamma  # gamma = paramÃ¨tre appris
```

Comparaison avec LayerNorm :

| | LayerNorm | RMSNorm |
|---|-----------|---------|
| Centrage (- moyenne) | Oui | Non |
| Normalisation | Oui | Oui |
| Vitesse | Plus lent | Plus rapide |
| UtilisÃ© par | GPT-2, BERT | LLaMA, Mistral |

RMSNorm est plus simple et tout aussi efficace.

---

### Pre-Norm vs Post-Norm

**Post-Norm** (Transformer original) :
```
x â†’ Attention â†’ Add(x) â†’ LayerNorm â†’ FFN â†’ Add â†’ LayerNorm
```

**Pre-Norm** (GPT moderne, LLaMA) :
```
x â†’ RMSNorm â†’ Attention â†’ Add(x) â†’ RMSNorm â†’ FFN â†’ Add(x)
```

Pre-Norm est plus stable Ã  l'entraÃ®nement, surtout pour les grands modÃ¨les.

---

### Connexions rÃ©siduelles

Le `+ x` aprÃ¨s chaque sous-couche :

```python
# Sans rÃ©siduel
x = attention(x)      # Si le gradient disparaÃ®t ici, tout est bloquÃ©

# Avec rÃ©siduel
x = x + attention(x)  # Le gradient passe toujours via la "route directe"
```

Pourquoi c'est crucial ?

```
Sans rÃ©siduel (6 couches):
  gradient Ã— 0.1 Ã— 0.1 Ã— 0.1 Ã— 0.1 Ã— 0.1 Ã— 0.1 = 0.000001 â†’ disparaÃ®t

Avec rÃ©siduel:
  le gradient a toujours un chemin direct vers chaque couche
```

---

### Le bloc Transformer complet

En combinant tout :

```python
def transformer_block(x):
    # Sous-couche 1 : Attention
    residual = x
    x = rmsnorm(x)
    x = multi_head_attention(x)
    x = residual + x              # connexion rÃ©siduelle

    # Sous-couche 2 : Feed-Forward
    residual = x
    x = rmsnorm(x)
    x = feed_forward_swiglu(x)
    x = residual + x              # connexion rÃ©siduelle

    return x
```

```
EntrÃ©e (n, 384)
    â†“
â”Œâ”€ RMSNorm â†’ Multi-Head Attention â”€â”
â”‚              â†“                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Add â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€ RMSNorm â†’ Feed-Forward (SwiGLU) â”
â”‚              â†“                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Add â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Sortie (n, 384)
```

</details>

### Questions de vÃ©rification

1. Pourquoi le FFN projette vers d_ff > d_model puis revient ?
2. Quel est l'avantage de RMSNorm sur LayerNorm ?
3. Que se passe-t-il sans connexions rÃ©siduelles dans un rÃ©seau profond ?

---

## Partie 8 : Architecture GPT complÃ¨te

On assemble tout. GPT empile **N blocs Transformer identiques**, avec un embedding en entrÃ©e et une projection vers le vocabulaire en sortie :

```
Tokens â†’ Embedding â†’ [Bloc Transformer Ã— N] â†’ RMSNorm â†’ LM Head â†’ ProbabilitÃ©s
```

Notre modÃ¨le : 6 couches, 6 tÃªtes, d_model=384, vocab_size=8192 â†’ **~10M paramÃ¨tres**.

<details>
<summary><strong>ğŸ“– Voir les dÃ©tails complets sur l'architecture GPT</strong></summary>

---

### Vue d'ensemble

```
Input IDs (batch, seq)
       â†“
Token Embedding          (vocab_size, d_model)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Block 1 â”‚
â”‚  â”Œâ”€ RMSNorm â†’ MHA â”€â”â”‚
â”‚  â””â”€â”€â”€ + rÃ©siduel â”€â”€â”€â”˜â”‚
â”‚  â”Œâ”€ RMSNorm â†’ FFN â”€â”â”‚
â”‚  â””â”€â”€â”€ + rÃ©siduel â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Transformer Block 2 â”‚
â”‚        ...           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Transformer Block 6 â”‚
â”‚        ...           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
RMSNorm finale
       â†“
LM Head (d_model â†’ vocab_size)
       â†“
Logits (batch, seq, vocab_size)
```

---

### Token Embedding

Convertit les IDs en vecteurs :

```python
self.token_emb = nn.Embedding(vocab_size, d_model)
# (batch, seq) â†’ (batch, seq, d_model)
```

Pas de positional embedding sÃ©parÃ© : RoPE est appliquÃ© directement dans l'attention.

---

### Les N blocs Transformer

Chaque bloc est identique (mÃªmes composants, mais poids diffÃ©rents) :

```python
self.layers = nn.ModuleList([
    TransformerBlock(d_model, n_heads, d_ff)
    for _ in range(n_layers)
])
```

Les couches basses captent des patterns simples (syntaxe, proximitÃ©), les couches hautes captent des patterns complexes (sÃ©mantique, raisonnement).

---

### RMSNorm finale

AprÃ¨s le dernier bloc, une normalisation finale avant la projection :

```python
self.final_norm = RMSNorm(d_model)
```

---

### LM Head (Language Model Head)

Projette les vecteurs vers le vocabulaire pour obtenir les probabilitÃ©s du prochain token :

```python
self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
# (batch, seq, d_model) â†’ (batch, seq, vocab_size)
```

---

### Weight Tying

Astuce : on **partage les poids** entre l'embedding et le LM Head :

```python
self.lm_head.weight = self.token_emb.weight
```

Pourquoi ?
- L'embedding convertit ID â†’ vecteur
- Le LM Head convertit vecteur â†’ ID
- Ce sont des opÃ©rations inverses â†’ partager les poids est logique
- Ã‰conomise `vocab_size Ã— d_model` paramÃ¨tres (3M dans notre cas)

---

### Forward pass complet

```python
def forward(self, input_ids):
    # 1. Embedding
    x = self.token_emb(input_ids)        # (B, S) â†’ (B, S, 384)

    # 2. N blocs Transformer
    for layer in self.layers:
        x = layer(x)                      # (B, S, 384) â†’ (B, S, 384)

    # 3. Normalisation finale
    x = self.final_norm(x)               # (B, S, 384)

    # 4. Projection vers le vocabulaire
    logits = self.lm_head(x)             # (B, S, 384) â†’ (B, S, 8192)

    return logits
```

---

### Comptage des paramÃ¨tres

```
Notre modÃ¨le (d_model=384, n_layers=6, n_heads=6, vocab_size=8192):

Token Embedding:     8192 Ã— 384          = 3,145,728

Par bloc Transformer (Ã—6):
  RMSNorm (att):     384                 = 384
  W_Q, W_K, W_V:     3 Ã— 384 Ã— 384      = 442,368
  W_O:                384 Ã— 384          = 147,456
  RMSNorm (ffn):     384                 = 384
  FFN (SwiGLU):      3 Ã— 384 Ã— 1024     = 1,179,648
  Sous-total bloc:                       = 1,770,240

6 blocs:             6 Ã— 1,770,240       = 10,621,440

RMSNorm finale:      384                 = 384
LM Head:             partagÃ© (0 extra)   = 0

TOTAL â‰ˆ 13.8M paramÃ¨tres
```

---

### Comparaison avec d'autres modÃ¨les

| ModÃ¨le | ParamÃ¨tres | Couches | d_model | TÃªtes |
|--------|-----------|---------|---------|-------|
| **Notre mini-GPT** | ~14M | 6 | 384 | 6 |
| GPT-2 Small | 117M | 12 | 768 | 12 |
| GPT-2 XL | 1.5B | 48 | 1600 | 25 |
| LLaMA 7B | 7B | 32 | 4096 | 32 |
| GPT-4 | ~1.8T (estimÃ©) | ? | ? | ? |

Le principe est identique, seule l'Ã©chelle change.

</details>

### Questions de vÃ©rification

1. Pourquoi partager les poids entre embedding et LM Head ?
2. Quel est le rÃ´le de la RMSNorm finale ?
3. Pourquoi les couches hautes captent des patterns plus complexes ?

---

## Prochaines parties
- **Partie 9** : EntraÃ®nement
- **Partie 10** : GÃ©nÃ©ration de texte et infÃ©rence

---

## Structure du projet

```
Creation_LLM/
â”œâ”€â”€ README.md              â† Ce fichier
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tokenizer.py       â† ImplÃ©mentation BPE
â”‚   â”œâ”€â”€ model.py           â† Architecture Transformer
â”‚   â”œâ”€â”€ train.py           â† Script d'entraÃ®nement
â”‚   â””â”€â”€ generate.py        â† Script de gÃ©nÃ©ration
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ CONCEPTS_LLM.md    â† RÃ©capitulatif technique
â””â”€â”€ data/                  â† DonnÃ©es d'entraÃ®nement
```

---

## Ressources

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Andrej Karpathy - Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [Attention Is All You Need (paper)](https://arxiv.org/abs/1706.03762)

# Pair programming contribution
