# Cr√©er un LLM from Scratch

Projet √©ducatif pour comprendre comment fonctionne un Large Language Model (LLM) de type GPT, √©tape par √©tape.

---

## Introduction

### C'est quoi un LLM ?

Un LLM (Large Language Model) est un mod√®le d'intelligence artificielle capable de comprendre et g√©n√©rer du texte. ChatGPT, Claude, LLaMA sont des exemples de LLMs.

### Comment √ßa fonctionne ?

Le principe est simple : **pr√©dire le mot suivant**.

```
Entr√©e:  "Le chat mange la"
Sortie:  "souris" (pr√©diction)
```

En r√©p√©tant cette pr√©diction, le mod√®le g√©n√®re du texte :

```
"Le chat" ‚Üí "mange"
"Le chat mange" ‚Üí "la"
"Le chat mange la" ‚Üí "souris"
...
```

### Le pipeline complet

```
Texte brut
    ‚Üì
1. TOKENIZATION  ‚Üí  Convertir texte en nombres
    ‚Üì
2. EMBEDDING     ‚Üí  Convertir nombres en vecteurs
    ‚Üì
3. ATTENTION     ‚Üí  Comprendre le contexte
    ‚Üì
4. G√âN√âRATION    ‚Üí  Pr√©dire le prochain token
```

---

## Partie 1 : Tokenization (BPE)

Un mod√®le ne comprend pas le texte, seulement des **nombres**. La tokenization convertit le texte en IDs :

```
"Bonjour le monde" ‚Üí [456, 12, 892] ‚Üí Le mod√®le
```

**BPE (Byte Pair Encoding)** fusionne les caract√®res fr√©quents pour cr√©er un vocabulaire efficace. Avantage : aucun mot n'est "inconnu", tout peut √™tre tokeniz√©.

<details>
<summary><strong>D√©tails complets sur BPE</strong></summary>

---

### Les 256 bytes de base

```
1 byte = 8 bits = 2‚Å∏ = 256 valeurs possibles (0 √† 255)
```

C'est la base de l'informatique. Chaque caract√®re a un code :

```
"A" = 65    "a" = 97    " " = 32
"√©" = 195 + 169 (2 bytes en UTF-8)
```

Tout texte peut √™tre repr√©sent√© en bytes ‚Üí BPE peut tokenizer **n'importe quel texte**.

---

### L'algorithme BPE

**Id√©e :** Fusionner les paires de caract√®res les plus fr√©quentes.

**Exemple** avec `"abab abab"` :

```
D√©part:  [a, b, a, b, ' ', a, b, a, b]  ‚Üí  9 tokens

Paire (a,b) appara√Æt 4 fois ‚Üí fusion en "ab"
         [ab, ab, ' ', ab, ab]          ‚Üí  5 tokens

Paire (ab,ab) appara√Æt 2 fois ‚Üí fusion en "abab"
         [abab, ' ', abab]              ‚Üí  3 tokens
```

**R√©sultat :** 9 tokens ‚Üí 3 tokens !

---

### vocab_size

```
vocab_size = 256 (bytes) + 4 (sp√©ciaux) + nombre de merges
```

| vocab_size | S√©quences | Mod√®le |
|------------|-----------|--------|
| Petit (1K) | Longues | L√©ger |
| Grand (50K) | Courtes | Lourd |

---

### Tokens sp√©ciaux

| Token | R√¥le |
|-------|------|
| `<pad>` | Remplissage |
| `<unk>` | Mot inconnu (jamais utilis√© avec BPE) |
| `<bos>` | D√©but de s√©quence |
| `<eos>` | Fin de s√©quence |

---

### BPE vs ancien syst√®me

```
Ancien:  "quinoa" ‚Üí <UNK>  (mot inconnu !)
BPE:     "quinoa" ‚Üí [qui][no][a]  (toujours d√©coupable)
```

</details>

### Questions de v√©rification

1. Pourquoi exactement 256 bytes de base ?
2. Un mot invent√© "xkzbrt" g√©n√®re-t-il une erreur avec BPE ?
3. Si j'augmente vocab_size, les s√©quences sont plus courtes ou plus longues ?

---

## Partie 2 : Embeddings

Les IDs de tokens sont juste des indices (456, 12, 892...). Le mod√®le a besoin de **vecteurs riches** pour capturer le sens. L'embedding convertit chaque ID en un vecteur de dimension `d_model` :

```
Token ID 456 ("chat") ‚Üí [0.2, -0.5, 0.8, ..., 0.1]  (384 dimensions)
```

Ces vecteurs sont **appris** pendant l'entra√Ænement : les mots similaires finissent proches dans l'espace vectoriel.

<details>
<summary><strong>üìñ Voir les d√©tails complets sur les Embeddings</strong></summary>

---

### La table d'embedding

C'est une matrice de taille `vocab_size √ó d_model` :

```
vocab_size = 8192 tokens
d_model = 384 dimensions

Table: 8192 √ó 384 = 3,145,728 param√®tres
```

Chaque ligne correspond √† un token :

```
ID 0   ‚Üí [0.1, 0.3, -0.2, ...]   (ligne 0)
ID 1   ‚Üí [0.5, -0.1, 0.7, ...]   (ligne 1)
...
ID 456 ‚Üí [0.2, -0.5, 0.8, ...]   (ligne 456 = "chat")
```

---

### Lookup (recherche)

L'embedding est juste une recherche dans la table :

```python
# Pseudo-code
embedding_table = matrix[vocab_size, d_model]

def embed(token_id):
    return embedding_table[token_id]  # Retourne la ligne
```

```
Entr√©e:  [456, 12, 892]  (3 token IDs)
Sortie:  [[...], [...], [...]]  (3 vecteurs de 384 dim)
         ‚Üí Tensor de shape (3, 384)
```

---

### Pourquoi d_model ?

`d_model` = dimension des vecteurs dans tout le mod√®le.

| d_model | Capacit√© | Param√®tres |
|---------|----------|------------|
| 128 | Faible | L√©ger |
| 384 | Moyenne | ~10M params |
| 768 | Haute | ~100M params |
| 4096 | Tr√®s haute | GPT-3 scale |

Plus `d_model` est grand, plus le mod√®le peut encoder d'information par token.

---

### Propri√©t√© : mots similaires = vecteurs proches

Apr√®s entra√Ænement, les embeddings capturent le sens :

```
distance("roi", "reine") < distance("roi", "voiture")
distance("chat", "chien") < distance("chat", "avion")
```

On peut m√™me faire de l'arithm√©tique :

```
embedding("roi") - embedding("homme") + embedding("femme") ‚âà embedding("reine")
```

---

### En r√©sum√©

```
Token IDs        ‚Üí  Embedding Table  ‚Üí  Vecteurs
[456, 12, 892]   ‚Üí  lookup           ‚Üí  (3, 384)
```

</details>

### Questions de v√©rification

1. Quelle est la taille de la table d'embedding si vocab_size=4096 et d_model=256 ?
2. L'embedding est-il appris ou fix√© √† l'avance ?
3. Pourquoi les mots similaires ont-ils des vecteurs proches ?

---

## Prochaines parties
- **Partie 3** : Attention (concept) - Comprendre pourquoi chaque mot regarde les autres
- **Partie 4** : Attention (calculs) - Les maths derri√®re Q, K, V
- **Partie 5** : Multi-Head Attention - Plusieurs "points de vue"
- **Partie 6** : Positional Encoding - Comment le mod√®le conna√Æt l'ordre des mots
- **Partie 7** : Feed-Forward et Normalisation
- **Partie 8** : Architecture GPT compl√®te
- **Partie 9** : Entra√Ænement
- **Partie 10** : G√©n√©ration de texte

---

## Structure du projet

```
Creation_LLM/
‚îú‚îÄ‚îÄ README.md              ‚Üê Ce fichier
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py       ‚Üê Impl√©mentation BPE
‚îÇ   ‚îú‚îÄ‚îÄ model.py           ‚Üê Architecture Transformer
‚îÇ   ‚îú‚îÄ‚îÄ train.py           ‚Üê Script d'entra√Ænement
‚îÇ   ‚îî‚îÄ‚îÄ generate.py        ‚Üê Script de g√©n√©ration
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ CONCEPTS_LLM.md    ‚Üê R√©capitulatif technique
‚îî‚îÄ‚îÄ data/                  ‚Üê Donn√©es d'entra√Ænement
```

---

## Ressources

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Andrej Karpathy - Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [Attention Is All You Need (paper)](https://arxiv.org/abs/1706.03762)

# Pair programming contribution
