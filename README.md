# CrÃ©er un LLM from Scratch

Projet Ã©ducatif pour comprendre comment fonctionne un Large Language Model (LLM) de type GPT, Ã©tape par Ã©tape.

---

## Introduction

### C'est quoi un LLM ?

Un LLM (Large Language Model) est un modÃ¨le d'intelligence artificielle capable de comprendre et gÃ©nÃ©rer du texte. ChatGPT, Claude, LLaMA sont des exemples de LLMs.

### Comment Ã§a fonctionne ?

Le principe est simple : **prÃ©dire le mot suivant**.

```
EntrÃ©e:  "Le chat mange la"
Sortie:  "souris" (prÃ©diction)
```

En rÃ©pÃ©tant cette prÃ©diction, le modÃ¨le gÃ©nÃ¨re du texte :

```
"Le chat" â†’ "mange"
"Le chat mange" â†’ "la"
"Le chat mange la" â†’ "souris"
...
```

### Le pipeline complet

```
Texte brut
    â†“
1. TOKENIZATION  â†’  Convertir texte en nombres
    â†“
2. EMBEDDING     â†’  Convertir nombres en vecteurs
    â†“
3. ATTENTION     â†’  Comprendre le contexte
    â†“
4. GÃ‰NÃ‰RATION    â†’  PrÃ©dire le prochain token
```

---

## Partie 1 : Tokenization (BPE)

Un modÃ¨le ne comprend pas le texte, seulement des **nombres**. La tokenization convertit le texte en IDs :

```
"Bonjour le monde" â†’ [456, 12, 892] â†’ Le modÃ¨le
```

**BPE (Byte Pair Encoding)** fusionne les caractÃ¨res frÃ©quents pour crÃ©er un vocabulaire efficace. Avantage : aucun mot n'est "inconnu", tout peut Ãªtre tokenizÃ©.

<details>
<summary><strong>ğŸ“– Voir les dÃ©tails complets sur BPE</strong></summary>

---

### Les 256 bytes de base

```
1 byte = 8 bits = 2â¸ = 256 valeurs possibles (0 Ã  255)
```

C'est la base de l'informatique. Chaque caractÃ¨re a un code :

```
"A" = 65    "a" = 97    " " = 32
"Ã©" = 195 + 169 (2 bytes en UTF-8)
```

Tout texte peut Ãªtre reprÃ©sentÃ© en bytes â†’ BPE peut tokenizer **n'importe quel texte**.

---

### L'algorithme BPE

**IdÃ©e :** Fusionner les paires de caractÃ¨res les plus frÃ©quentes.

**Exemple** avec `"abab abab"` :

```
DÃ©part:  [a, b, a, b, ' ', a, b, a, b]  â†’  9 tokens

Paire (a,b) apparaÃ®t 4 fois â†’ fusion en "ab"
         [ab, ab, ' ', ab, ab]          â†’  5 tokens

Paire (ab,ab) apparaÃ®t 2 fois â†’ fusion en "abab"
         [abab, ' ', abab]              â†’  3 tokens
```

**RÃ©sultat :** 9 tokens â†’ 3 tokens !

---

### vocab_size

```
vocab_size = 256 (bytes) + 4 (spÃ©ciaux) + nombre de merges
```

| vocab_size | SÃ©quences | ModÃ¨le |
|------------|-----------|--------|
| Petit (1K) | Longues | LÃ©ger |
| Grand (50K) | Courtes | Lourd |

---

### Tokens spÃ©ciaux

| Token | RÃ´le |
|-------|------|
| `<pad>` | Remplissage |
| `<unk>` | Mot inconnu (jamais utilisÃ© avec BPE) |
| `<bos>` | DÃ©but de sÃ©quence |
| `<eos>` | Fin de sÃ©quence |

---

### BPE vs ancien systÃ¨me

```
Ancien:  "quinoa" â†’ <UNK>  (mot inconnu !)
BPE:     "quinoa" â†’ [qui][no][a]  (toujours dÃ©coupable)
```

</details>

### Questions de vÃ©rification

1. Pourquoi exactement 256 bytes de base ?
2. Un mot inventÃ© "xkzbrt" gÃ©nÃ¨re-t-il une erreur avec BPE ?
3. Si j'augmente vocab_size, les sÃ©quences sont plus courtes ou plus longues ?

---

## Prochaines parties

- **Partie 2** : Embeddings - Convertir les IDs en vecteurs
- **Partie 3** : Attention (concept) - Comprendre pourquoi chaque mot regarde les autres
- **Partie 4** : Attention (calculs) - Les maths derriÃ¨re Q, K, V
- **Partie 5** : Multi-Head Attention - Plusieurs "points de vue"
- **Partie 6** : Positional Encoding - Comment le modÃ¨le connaÃ®t l'ordre des mots
- **Partie 7** : Feed-Forward et Normalisation
- **Partie 8** : Architecture GPT complÃ¨te
- **Partie 9** : EntraÃ®nement
- **Partie 10** : GÃ©nÃ©ration de texte

---

## Structure du projet

```
Creation_LLM/
â”œâ”€â”€ README.md              â† Ce fichier
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tokenizer.py       â† ImplÃ©mentation BPE
â”‚   â”œâ”€â”€ model.py           â† Architecture Transformer
â”‚   â”œâ”€â”€ train.py           â† Script d'entraÃ®nement
â”‚   â””â”€â”€ generate.py        â† Script de gÃ©nÃ©ration
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ CONCEPTS_LLM.md    â† RÃ©capitulatif technique
â””â”€â”€ data/                  â† DonnÃ©es d'entraÃ®nement
```

---

## Ressources

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Andrej Karpathy - Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [Attention Is All You Need (paper)](https://arxiv.org/abs/1706.03762)
