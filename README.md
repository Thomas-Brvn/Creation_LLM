# CrÃ©er un LLM from Scratch

Projet Ã©ducatif pour comprendre comment fonctionne un Large Language Model (LLM) de type GPT, Ã©tape par Ã©tape.

---

## Introduction

### C'est quoi un LLM ?

Un LLM (Large Language Model) est un modÃ¨le d'intelligence artificielle capable de comprendre et gÃ©nÃ©rer du texte. ChatGPT, Claude, LLaMA sont des exemples de LLMs.

### Comment Ã§a fonctionne ?

Le principe est simple : **prÃ©dire le mot suivant**.

```
EntrÃ©e:  "Le chat mange la"
Sortie:  "souris" (prÃ©diction)
```

En rÃ©pÃ©tant cette prÃ©diction, le modÃ¨le gÃ©nÃ¨re du texte :

```
"Le chat" â†’ "mange"
"Le chat mange" â†’ "la"
"Le chat mange la" â†’ "souris"
...
```

### Le pipeline complet

```
Texte brut
    â†“
1. TOKENIZATION  â†’  Convertir texte en nombres
    â†“
2. EMBEDDING     â†’  Convertir nombres en vecteurs
    â†“
3. ATTENTION     â†’  Comprendre le contexte
    â†“
4. GÃ‰NÃ‰RATION    â†’  PrÃ©dire le prochain token
```

---

## Partie 1 : Tokenization (BPE)

Un modÃ¨le ne comprend pas le texte, seulement des **nombres**. La tokenization convertit le texte en IDs :

```
"Bonjour le monde" â†’ [456, 12, 892] â†’ Le modÃ¨le
```

**BPE (Byte Pair Encoding)** fusionne les caractÃ¨res frÃ©quents pour crÃ©er un vocabulaire efficace. Avantage : aucun mot n'est "inconnu", tout peut Ãªtre tokenizÃ©.

<details>
<summary><strong>DÃ©tails complets sur BPE</strong></summary>

---

### Les 256 bytes de base

```
1 byte = 8 bits = 2â¸ = 256 valeurs possibles (0 Ã  255)
```

C'est la base de l'informatique. Chaque caractÃ¨re a un code :

```
"A" = 65    "a" = 97    " " = 32
"Ã©" = 195 + 169 (2 bytes en UTF-8)
```

Tout texte peut Ãªtre reprÃ©sentÃ© en bytes â†’ BPE peut tokenizer **n'importe quel texte**.

---

### L'algorithme BPE

**IdÃ©e :** Fusionner les paires de caractÃ¨res les plus frÃ©quentes.

**Exemple** avec `"abab abab"` :

```
DÃ©part:  [a, b, a, b, ' ', a, b, a, b]  â†’  9 tokens

Paire (a,b) apparaÃ®t 4 fois â†’ fusion en "ab"
         [ab, ab, ' ', ab, ab]          â†’  5 tokens

Paire (ab,ab) apparaÃ®t 2 fois â†’ fusion en "abab"
         [abab, ' ', abab]              â†’  3 tokens
```

**RÃ©sultat :** 9 tokens â†’ 3 tokens !

---

### vocab_size

```
vocab_size = 256 (bytes) + 4 (spÃ©ciaux) + nombre de merges
```

| vocab_size | SÃ©quences | ModÃ¨le |
|------------|-----------|--------|
| Petit (1K) | Longues | LÃ©ger |
| Grand (50K) | Courtes | Lourd |

---

### Tokens spÃ©ciaux

| Token | RÃ´le |
|-------|------|
| `<pad>` | Remplissage |
| `<unk>` | Mot inconnu (jamais utilisÃ© avec BPE) |
| `<bos>` | DÃ©but de sÃ©quence |
| `<eos>` | Fin de sÃ©quence |

---

### BPE vs ancien systÃ¨me

```
Ancien:  "quinoa" â†’ <UNK>  (mot inconnu !)
BPE:     "quinoa" â†’ [qui][no][a]  (toujours dÃ©coupable)
```

</details>

### Questions de vÃ©rification

1. Pourquoi exactement 256 bytes de base ?
2. Un mot inventÃ© "xkzbrt" gÃ©nÃ¨re-t-il une erreur avec BPE ?
3. Si j'augmente vocab_size, les sÃ©quences sont plus courtes ou plus longues ?

---

## Partie 2 : Embeddings

Les IDs de tokens sont juste des indices (456, 12, 892...). Le modÃ¨le a besoin de **vecteurs riches** pour capturer le sens. L'embedding convertit chaque ID en un vecteur de dimension `d_model` :

```
Token ID 456 ("chat") â†’ [0.2, -0.5, 0.8, ..., 0.1]  (384 dimensions)
```

Ces vecteurs sont **appris** pendant l'entraÃ®nement : les mots similaires finissent proches dans l'espace vectoriel.

<details>
<summary><strong>ğŸ“– Voir les dÃ©tails complets sur les Embeddings</strong></summary>

---

### La table d'embedding

C'est une matrice de taille `vocab_size Ã— d_model` :

```
vocab_size = 8192 tokens
d_model = 384 dimensions

Table: 8192 Ã— 384 = 3,145,728 paramÃ¨tres
```

Chaque ligne correspond Ã  un token :

```
ID 0   â†’ [0.1, 0.3, -0.2, ...]   (ligne 0)
ID 1   â†’ [0.5, -0.1, 0.7, ...]   (ligne 1)
...
ID 456 â†’ [0.2, -0.5, 0.8, ...]   (ligne 456 = "chat")
```

---

### Lookup (recherche)

L'embedding est juste une recherche dans la table :

```python
# Pseudo-code
embedding_table = matrix[vocab_size, d_model]

def embed(token_id):
    return embedding_table[token_id]  # Retourne la ligne
```

```
EntrÃ©e:  [456, 12, 892]  (3 token IDs)
Sortie:  [[...], [...], [...]]  (3 vecteurs de 384 dim)
         â†’ Tensor de shape (3, 384)
```

---

### Pourquoi d_model ?

`d_model` = dimension des vecteurs dans tout le modÃ¨le.

| d_model | CapacitÃ© | ParamÃ¨tres |
|---------|----------|------------|
| 128 | Faible | LÃ©ger |
| 384 | Moyenne | ~10M params |
| 768 | Haute | ~100M params |
| 4096 | TrÃ¨s haute | GPT-3 scale |

Plus `d_model` est grand, plus le modÃ¨le peut encoder d'information par token.

---

### PropriÃ©tÃ© : mots similaires = vecteurs proches

AprÃ¨s entraÃ®nement, les embeddings capturent le sens :

```
distance("roi", "reine") < distance("roi", "voiture")
distance("chat", "chien") < distance("chat", "avion")
```

On peut mÃªme faire de l'arithmÃ©tique :

```
embedding("roi") - embedding("homme") + embedding("femme") â‰ˆ embedding("reine")
```

---

### En rÃ©sumÃ©

```
Token IDs        â†’  Embedding Table  â†’  Vecteurs
[456, 12, 892]   â†’  lookup           â†’  (3, 384)
```

</details>

### Questions de vÃ©rification

1. Quelle est la taille de la table d'embedding si vocab_size=4096 et d_model=256 ?
2. L'embedding est-il appris ou fixÃ© Ã  l'avance ?
3. Pourquoi les mots similaires ont-ils des vecteurs proches ?

---

## Prochaines parties
- **Partie 3** : Attention (concept) - Comprendre pourquoi chaque mot regarde les autres
- **Partie 4** : Attention (calculs) - Les maths derriÃ¨re Q, K, V
- **Partie 5** : Multi-Head Attention - Plusieurs "points de vue"
- **Partie 6** : Positional Encoding - Comment le modÃ¨le connaÃ®t l'ordre des mots
- **Partie 7** : Feed-Forward et Normalisation
- **Partie 8** : Architecture GPT complÃ¨te
- **Partie 9** : EntraÃ®nement
- **Partie 10** : GÃ©nÃ©ration de texte

---

## Structure du projet

```
Creation_LLM/
â”œâ”€â”€ README.md              â† Ce fichier
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tokenizer.py       â† ImplÃ©mentation BPE
â”‚   â”œâ”€â”€ model.py           â† Architecture Transformer
â”‚   â”œâ”€â”€ train.py           â† Script d'entraÃ®nement
â”‚   â””â”€â”€ generate.py        â† Script de gÃ©nÃ©ration
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ CONCEPTS_LLM.md    â† RÃ©capitulatif technique
â””â”€â”€ data/                  â† DonnÃ©es d'entraÃ®nement
```

---

## Ressources

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Andrej Karpathy - Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [Attention Is All You Need (paper)](https://arxiv.org/abs/1706.03762)
